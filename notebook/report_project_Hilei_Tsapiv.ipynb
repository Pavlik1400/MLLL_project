{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MNIST 2 digits classification using 4 methods\n",
    "\n",
    "### Authors:\n",
    "- Pavlo Hilei\n",
    "- Volodymyr Tsapiv\n",
    "\n",
    "### Approaches:\n",
    "- Artificial Neural Network (NN) [Hilei Pavlo]\n",
    "- K-nearest neighbors (KNN) [Volodymyr Tsapiv]\n",
    "- Support Vector Machine (SVM) [Hilei Pavlo]\n",
    "- Logistic Regression [Volodymyr Tsapiv]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mnist dataset consists of hand-written digits. The objective is to classify them correctly using ML approaches. In the current project we have to classify only 2 digits.\n",
    "\n",
    "Due to the problem statement, we have to take such digits $a$, $b$, that sum of days of our birthdays form $\\overline{ab}$\n",
    "\n",
    "## 1. Data exploration and preparation\n",
    "\n",
    "We use dataset from `torchvision.datasets` library. Let's load and check it out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# let's import stuff\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List, Dict, Callable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.datasets as datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the whole dataset we take subset consisting of 2 passed digits, normalize photos. Also we split train to train/cross-validation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Dataset:\n",
    "    train_data: np.ndarray\n",
    "    train_targets: np.ndarray\n",
    "    cv_data: np.ndarray\n",
    "    cv_targets: np.ndarray\n",
    "    test_data: np.ndarray\n",
    "    test_targets: np.ndarray\n",
    "\n",
    "\n",
    "def load(digit1: int, digit2: int) -> Dataset:\n",
    "    train_cv_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "    train_cv_data, train_cv_targets = transform(train_cv_dataset, digit1, digit2)\n",
    "    train_data, cv_data, train_targets, cv_targets = train_test_split(train_cv_data, train_cv_targets, test_size=0.2)\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "    test_data, test_targets = transform(test_dataset, digit1, digit2)\n",
    "    return Dataset(train_data, train_targets, cv_data, cv_targets, test_data, test_targets)\n",
    "\n",
    "\n",
    "def transform(dataset: datasets.MNIST, digit1: int, digit2: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # data = dataset.data.numpy().astype(np.float64) / 255\n",
    "    # normalize data\n",
    "    data = dataset.data.numpy().astype(np.float64)\n",
    "    data = (data - np.mean(data)) / np.std(data)\n",
    "    targets = dataset.targets.numpy()\n",
    "\n",
    "    # filter by digits\n",
    "    mask = np.bitwise_or(targets == digit1, targets == digit2)\n",
    "    data = data[mask]\n",
    "    targets = targets[mask]\n",
    "    return data.reshape(len(data), -1), targets == digit1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's calculate `dig1` and `dig2`, load data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digits are: 8, 5\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.6%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31.0%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "85.4%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "112.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Dataset with digits: 8, 5\n",
      "Size of train: 9017, ratio: 1.0\n",
      "Size of cv: 2255, ratio: 1.0\n",
      "Size of test: 1866, ratio: 1.0\n"
     ]
    }
   ],
   "source": [
    "b1, b2 = 30, 28  # 30 and 28 are days of birthdays\n",
    "n = b1 + b2\n",
    "dig1 = n % 10\n",
    "dig2 = n // 10\n",
    "print(f\"Digits are: {dig1}, {dig2}\")\n",
    "dataset = load(dig1, dig2)\n",
    "\n",
    "print(f\"Dataset with digits: {dig1}, {dig2}\")\n",
    "print(f\"Size of train: {len(dataset.train_data)}, \"\n",
    "      f\"ratio: {len(dataset.train_targets == dig1) / len(dataset.train_targets == dig2)}\")\n",
    "print(f\"Size of cv: {len(dataset.cv_data)}, \"\n",
    "      f\"ratio: {len(dataset.cv_targets == dig1) / len(dataset.cv_targets == dig2)}\")\n",
    "print(f\"Size of test: {len(dataset.test_data)}, \"\n",
    "      f\"ratio: {len(dataset.test_targets == dig1) / len(dataset.test_targets == dig2)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's vizualize our data:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of images:\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 2592x288 with 6 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_mnist_imgs(imgs: List[np.ndarray], is_normalized=True, targets=None):\n",
    "    if is_normalized:\n",
    "        for idx, img in enumerate(imgs):\n",
    "            img = img - np.min(img)\n",
    "            img = img / np.max(img)\n",
    "            img = img.reshape((28, 28))\n",
    "            imgs[idx] = img\n",
    "    fig, axes = plt.subplots(1, len(imgs))\n",
    "    w, h = fig.get_size_inches()\n",
    "    fig.set_size_inches(w * len(imgs), h)\n",
    "    for idx, img in enumerate(imgs):\n",
    "        axes[idx].imshow(img, cmap=\"Greys\")\n",
    "        if targets is not None:\n",
    "            axes[idx].set_title(targets[idx])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"Example of images:\")\n",
    "bool_to_dig = {True: dig1, False: dig2}\n",
    "show_mnist_imgs(list(dataset.train_data[:6]),\n",
    "                targets=[f\"target: {bool_to_dig[v]}\" for v in dataset.train_targets[:6]])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Good. Data seems OK. now let's start with approaches.\n",
    "\n",
    "Since we're allowed and encoureged to use libraries, and 3 of 4 methods are implemented in sklean lib, let's create some generic function that can tune paramters for best performace.\n",
    "\n",
    "Function `select_model` takes dataset, model, and parameters. Then:\n",
    "1. With the help of `grid_search` it calculates all possible combinations of parameters.\n",
    "2. Trains the model.\n",
    "3. Calculates accuracy on cross-validation dataset.\n",
    "4. Saves model and parameters.\n",
    "5. Chooses best model by accuracy score.\n",
    "6. Runs best model on test dataset to score how well it performs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "\n",
    "def grid_search(params):\n",
    "    return [dict(zip(params, v)) for v in product(*params.values())]\n",
    "\n",
    "\n",
    "def select_model(dataset: Dataset, model_prototype, parameters: Dict):\n",
    "    parameters_set = grid_search(parameters)\n",
    "    scores = []\n",
    "    models = []\n",
    "\n",
    "    for p in parameters_set:\n",
    "        model = model_prototype(**p)\n",
    "        model.fit(dataset.train_data, dataset.train_targets)\n",
    "\n",
    "        score = model.score(dataset.cv_data, dataset.cv_targets)\n",
    "        print(f\"Cross-validation accuracy={score:.4f}, parameters={p}\")\n",
    "\n",
    "        models.append(model)\n",
    "        scores.append(score)\n",
    "\n",
    "    best = np.argmax(scores)\n",
    "    best_model = models[best]\n",
    "\n",
    "    score = best_model.score(dataset.test_data, dataset.test_targets)\n",
    "    print(f\"Test accuracy={score:.4f}, parameters={parameters_set[best]}\")\n",
    "\n",
    "    return best_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Logistic Regression\n",
    "\n",
    "Logistic regression is the appropriate regression analysis to conduct when the dependent variable is **dichotomous** (binary). Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.\n",
    "\n",
    "Within machine learning, logistic regression belongs to the family of **supervised** machine learning models. It is also considered a **discriminative model**, which means that it attempts to distinguish between classes (or categories). Unlike a generative algorithm, such as naïve bayes, it cannot, as the name implies, generate information, such as an image, of the class that it is trying to predict (e.g. a picture of a cat).\n",
    "\n",
    "Logistic regression maximizes the **log likelihood** function to determine the beta coefficients of the model. This changes slightly under the context of machine learning. Within machine learning, the negative log likelihood used as the loss function, using the process of gradient descent to find the global maximum. This is just another way to arrive at the same estimations discussed below."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let’s start by mentioning the formula of logistic function:\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{\\theta^Tx}}\n",
    "$$\n",
    "\n",
    "There are obviously some similarities between the formula above and linear regression:\n",
    "$$\n",
    "y(x) = \\theta^Tx\n",
    "$$\n",
    "The key is that we want linear regression to output the probability $p = \\theta^Tx$ that has to be in 0 to 1 range, but obviously it is not the case in general. To overcome this issue we take \"odds\" of p:\n",
    "$$\n",
    "\\frac{p}{1 - p} = \\theta^Tx\n",
    "$$\n",
    "We know that odds can always be positive which means the range will always be (0,+$\\infty$). Odds are nothing but the ratio of the probability of success and probability of failure. Still, left-hand side cannot obtain all the values right-hand side can; we fix by taking log of odds.\n",
    "$$\n",
    "\\log{\\frac{p}{1 - p}} = \\theta^Tx\n",
    "$$\n",
    "Now, we can just solve for p and make sure that:\n",
    "$$\n",
    "p = \\sigma(x) = \\frac{1}{1 + e^{\\theta^Tx}}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Cost function in logistic regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In linear regression, we use the **mean squared error** which was the difference between y_predicted and y_actual and this is derived from the maximum likelihood estimator:\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{n}\\sum_{i = 1}^n (\\hat y_i - y_i)^2\n",
    "$$\n",
    "In logistic regression $\\hat y_i$ is a non-linear function. If we use this in the above MSE equation then it will give a non-convex graph with many local minima.\n",
    "\n",
    "Let’s start by defining our likelihood function. We now know that the labels are binary which means they can be either yes/no or pass/fail etc. We can also say we have two outcomes success and failure. This means we can interpret each label as Bernoulli random variable.\n",
    "\n",
    "A random experiment whose outcomes are of two types, success S and failure F, occurring with probabilities p and q respectively is called a Bernoulli trial. If for this experiment a random variable X is defined such that it takes value 1 when S occurs and 0 if F occurs, then X follows a Bernoulli Distribution.\n",
    "$$\n",
    "Y \\sim Bernoulli(p)\n",
    "$$\n",
    "where $p$ is sigmoid fuction.\n",
    "$$\n",
    "P[Y=y|X=x] = \\sigma(\\theta^Tx)^y(1 - \\sigma(\\theta^Tx))^{1 - y}\n",
    "$$\n",
    "Now for n observations,\n",
    "$$\n",
    "L(\\theta) = \\prod_{i = 1}^n \\sigma(\\theta^Tx_i)^{y_i}(1 - \\sigma(\\theta^Tx_i))^{1 - y_i}\n",
    "$$\n",
    "We need a value for theta which will maximize this likelihood function. To make our calculations easier we multiply the log on both sides. The function we get is also called the log-likelihood function or sum of the log conditional probability\n",
    "$$\n",
    "\\log L(\\theta) = \\sum_{i = 1}^n y_i \\cdot \\log [\\sigma(\\theta^Tx_i)] + (1 - y_i) \\cdot \\log [(1 - \\sigma(\\theta^Tx_i))]\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Optimization\n",
    "\n",
    "1. Use chain rule and break the partial derivative of log-likelihood.\n",
    "$$\n",
    "\\frac{\\partial LL(\\theta)}{\\partial \\theta_j} = \\frac{\\partial LL(\\theta)}{\\partial p} \\cdot \\frac{\\partial p}{\\partial \\theta_j}\n",
    "$$\n",
    "where $p = \\sigma[\\theta^Tx]$\n",
    "$$\n",
    "= \\frac{\\partial LL(\\theta)}{\\partial p} \\cdot \\frac{\\partial p}{\\partial z} \\cdot \\frac{\\partial z}{\\partial \\theta_j}\n",
    "$$\n",
    "where $z = \\theta^Tx$\n",
    "\n",
    "2. Find derivative of log-likelihood w.r.t $p$\n",
    "$$\n",
    "LL(\\theta) = y \\cdot \\log p + (1 - y) \\cdot \\log (1 - p)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial LL(\\theta)}{\\partial p} = \\frac{y}{p} + \\frac{1 - y}{1 - p}\n",
    "$$\n",
    "3. Find derivative of $p$ w.r.t $z$\n",
    "$$ p = \\sigma(z) $$\n",
    "$$\n",
    "\\frac{\\partial p}{\\partial z} = \\frac{\\partial \\sigma(z)}{\\partial z} = \\sigma(z) \\cdot (1 - \\sigma(z))\n",
    "$$\n",
    "4. Find derivate of $z$ w.r.t $\\theta$\n",
    "$$ z = \\theta^Tx $$\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial \\theta_j} = x_j\n",
    "$$\n",
    "5. Put all the derivatives in equation 1\n",
    "\n",
    "$$\n",
    "\\frac{\\partial LL(\\theta)}{\\partial \\theta_j} = \\frac{\\partial LL(\\theta)}{\\partial p} \\cdot \\frac{\\partial p}{\\partial z} \\cdot \\frac{\\partial z}{\\partial \\theta_j} = [\\frac{y}{p} + \\frac{1 - y}{1 - p}] \\cdot \\sigma(z) \\cdot (1 - \\sigma(z)) \\cdot x_j\n",
    "$$\n",
    "$$\n",
    "= [\\frac{y}{p} + \\frac{1 - y}{1 - p}] \\cdot p[1 - p] \\cdot x_j = [y(1 - p) - p(1 - y)] \\cdot x_j = (y - p) \\cdot x_j\n",
    "$$\n",
    "$$= (y - \\sigma(\\theta^Tx)) \\cdot x_j $$\n",
    "\n",
    "We can clearly see that there are no explicite solution for $\\theta$ unlike in linear regression. Thus, we use **gradient ascent** in order to find the optimal $\\theta$. Using equation 5 we obtain update rule:\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_{t} + (y - \\sigma(\\theta_t^Tx)) \\cdot x\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation accuracy=0.9601, parameters={'solver': 'saga', 'max_iter': 200, 'C': 1}\n",
      "Cross-validation accuracy=0.9605, parameters={'solver': 'saga', 'max_iter': 200, 'C': 0.1}\n",
      "Cross-validation accuracy=0.9588, parameters={'solver': 'saga', 'max_iter': 200, 'C': 0.03}\n",
      "Cross-validation accuracy=0.9579, parameters={'solver': 'lbfgs', 'max_iter': 200, 'C': 1}\n",
      "Cross-validation accuracy=0.9610, parameters={'solver': 'lbfgs', 'max_iter': 200, 'C': 0.1}\n",
      "Cross-validation accuracy=0.9596, parameters={'solver': 'lbfgs', 'max_iter': 200, 'C': 0.03}\n",
      "Cross-validation accuracy=0.9579, parameters={'solver': 'liblinear', 'max_iter': 200, 'C': 1}\n",
      "Cross-validation accuracy=0.9610, parameters={'solver': 'liblinear', 'max_iter': 200, 'C': 0.1}\n",
      "Cross-validation accuracy=0.9588, parameters={'solver': 'liblinear', 'max_iter': 200, 'C': 0.03}\n",
      "Cross-validation accuracy=0.9579, parameters={'solver': 'newton-cg', 'max_iter': 200, 'C': 1}\n",
      "Cross-validation accuracy=0.9614, parameters={'solver': 'newton-cg', 'max_iter': 200, 'C': 0.1}\n",
      "Cross-validation accuracy=0.9596, parameters={'solver': 'newton-cg', 'max_iter': 200, 'C': 0.03}\n",
      "Cross-validation accuracy=0.9596, parameters={'solver': 'sag', 'max_iter': 200, 'C': 1}\n",
      "Cross-validation accuracy=0.9610, parameters={'solver': 'sag', 'max_iter': 200, 'C': 0.1}\n",
      "Cross-validation accuracy=0.9588, parameters={'solver': 'sag', 'max_iter': 200, 'C': 0.03}\n",
      "Test accuracy=0.9561, parameters={'solver': 'newton-cg', 'max_iter': 200, 'C': 0.1}\n"
     ]
    },
    {
     "data": {
      "text/plain": "LogisticRegression(C=0.1, max_iter=200, solver='newton-cg')",
      "text/html": "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.1, max_iter=200, solver=&#x27;newton-cg&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.1, max_iter=200, solver=&#x27;newton-cg&#x27;)</pre></div></div></div></div></div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "parameters = {'solver': ['saga', 'lbfgs', 'liblinear', 'newton-cg', 'sag'], 'max_iter': [200], 'C': [1, 0.1, 0.03]}\n",
    "select_model(dataset, LogisticRegression, parameters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic regression pros\n",
    "\n",
    "**Pros**:\n",
    "- Easy to implement\n",
    "- The predicted parameters (trained weights) give inference about the importance of each feature. (Highly interpretable)\n",
    "- This algorithm allows models to be updated easily to reflect new data, unlike decision trees or support vector machines. The update can be done using stochastic gradient descent.\n",
    "- Logistic regression outputs well-calibrated probabilities along with classification results. This is an advantage over models that only give the final classification as results.\n",
    "- In a low dimensional dataset having a sufficient number of training examples, logistic regression is less prone to over-fitting.\n",
    "\n",
    "**Cons**:\n",
    "- Non-linear problems can't be solved with logistic regression since it has a linear decision surface. Linearly separable data is rarely found in real world scenarios.\n",
    "- The training features are known as independent variables. Logistic Regression requires moderate or no multicollinearity between independent variables. Repetition of information could lead to wrong training of parameters (weights) during minimizing the cost function.\n",
    "- On high dimensional datasets with an insufficient number of data samples, the model is prone to over-fit on the training set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test logistic regression accuracy on test data - 95.6%\n",
    "\n",
    "Note that best cross-validation accuracy we get with more strict regularization 0.1 (default is 1.0). It might be due to the fact that our data is high-dimensional (784d). I believe that results can be improved by apply PCA before classification."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. K-nearest neighbors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "K-nearest Neighbor is a **supervised classification** algorithm that is based on predicting data by finding the similarities to the underlying data. KNN is most widely used for classification problems, but can also be used to solve regression problems. The original assumption is the data exist in forms of clusters or exist in close proximity. KNN is a non-parametric algorithm, which means it does not attempt to make an assumption on the data, for example it does not care if the data is normally distributed or not."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Classification rule"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Informally:\n",
    "\n",
    "For a test input x, assign the most common label amongst its k most similar training inputs\n",
    "\n",
    "Formally:\n",
    "\n",
    "Let $x$ to be test point. Define the set of the k nearest neighbors of $x$ as $S_x$. $S_x$ is defined as $S_x \\subseteq D$ s.t. $|S_x|=k$ and\n",
    "$$\n",
    "\\forall (x', y') \\in D \\setminus S_x \\quad dist(x, x') \\ge \\max_{(x'', y'') \\in S_x} dist(x, x''),\n",
    "$$\n",
    "(i.e. every point in D but not in $S_x$ is at least as far away from $x$ as the furthest point in $S_x$). We can then define the classifier h() as a function returning the most common label in $S_x$:\n",
    "$$\n",
    "h(x) = mode({y'': (x'', y'') \\in S_x}),\n",
    "$$\n",
    "where $mode(\\cdot)$ means to select the label of the highest occurrence."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Distance function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The k-nearest neighbor classifier fundamentally relies on a distance metric. The better that metric reflects label similarity, the better the classified will be. The most common choice is the Minkowski distance\n",
    "$$\n",
    "dist(x, z) = (\\sum_{r=1}^d |x_r - z_r|^p)^{\\frac{1}{p}}\n",
    "$$\n",
    "- p = 1: Manhattan Distance ($l_1$-norm)\n",
    "- p = 2: Euclidean Distance ($l_2$-norm)\n",
    "- p $\\to \\infty$: Maximum Norm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1-NN Convergence proof\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cover and Hart 1967: As $n \\to \\infty$, the 1-NN error is no more than twice the error of the Bayes Optimal classifier. (Similar guarantees hold for k>1.)\n",
    "\n",
    "Let $x_{NN}$ be the nearest neighbor of our test point $x_t$. As $n \\to \\infty$, dist($x_{NN},x_t$) $\\to 0$, i.e. $x_{NN} \\to x_t$. (This means the nearest neighbor is identical to $x_t$.) You return the label of $x_{NN}$. What is the probability that this is not the label of x? (This is the probability of drawing two different label of x)\n",
    "$$\n",
    "\\varepsilon_{NN} = P(y^*|x_t)(1 - P(y^*|x_{NN})) + P(y^*|x_{NN})(1 - P(y^*|x_t)) \\le (1 - P(y^*|x_{NN})) + (1 - P(y^*|x_t)) = 2(1 - P(y^*|x_t)) = 2\\varepsilon_{BayesOpt}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Curse of dimensionality\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Imagine $X=[0,1]^d$, and k=10 and all training data is sampled uniformly with X, i.e. $\\forall i,x_i \\in [0,1]^d$. Let $l$ be the edge length of the smallest hyper-cube that contains all k-nearest neighbor of a test point. Then $l^d \\approx \\frac{k}{n}$ and $l \\approx (\\frac{k}{n})^{1/d}$\n",
    "Let's take $n = 1000$ and how $l$ changes as d grows:\n",
    "$$\n",
    "d \\quad l \\\\\n",
    "2 \\quad 0.1 \\\\\n",
    "10 \\quad 0.63 \\\\\n",
    "100 \\quad 0.955 \\\\\n",
    "1000 \\quad 0.9954\n",
    "$$\n",
    "Almost the entire space is needed to find the 10-NN.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation accuracy=0.9858, parameters={'n_neighbors': 11}\n",
      "Cross-validation accuracy=0.9854, parameters={'n_neighbors': 13}\n",
      "Cross-validation accuracy=0.9845, parameters={'n_neighbors': 15}\n",
      "Cross-validation accuracy=0.9836, parameters={'n_neighbors': 17}\n",
      "Cross-validation accuracy=0.9840, parameters={'n_neighbors': 19}\n",
      "Cross-validation accuracy=0.9831, parameters={'n_neighbors': 21}\n",
      "Cross-validation accuracy=0.9823, parameters={'n_neighbors': 23}\n",
      "Cross-validation accuracy=0.9805, parameters={'n_neighbors': 25}\n",
      "Cross-validation accuracy=0.9800, parameters={'n_neighbors': 27}\n",
      "Cross-validation accuracy=0.9800, parameters={'n_neighbors': 29}\n",
      "Test accuracy=0.9877, parameters={'n_neighbors': 11}\n"
     ]
    },
    {
     "data": {
      "text/plain": "KNeighborsClassifier(n_neighbors=11)",
      "text/html": "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=11)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=11)</pre></div></div></div></div></div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "parameters = {'n_neighbors': list(range(11, 30, 2))}\n",
    "select_model(dataset, KNeighborsClassifier, parameters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### k-NN pros and cons\n",
    "\n",
    "**Pros**:\n",
    "- intuitive and simple: k-NN algorithm is very simple to understand and equally easy to implement.\n",
    "- no assumptions: K-NN is a non-parametric algorithm which means there are assumptions to be met to implement it.\n",
    "- no training step: Given it’s an instance-based learning; k-NN is a memory-based approach. The classifier immediately adapts as we collect new training data.\n",
    "- easy to implement for multi-class problem: Most of the classifier algorithms are easy to implement for binary problems and needs effort to implement for multi class.\n",
    "- can be used both for classification and regression\n",
    "- only one hyper-parameter\n",
    "- variety of distance criteria to be choose from: Euclidean dist, Hamming dist, Manhattan dist and Minkowski dist as most general\n",
    "\n",
    "**Cons**:\n",
    "- slow algorithm: k-NN might be very easy to implement but as dataset grows efficiency or speed of algorithm declines very fast.\n",
    "- curse of dimensionality: k-NN works well with small number of input variables but as the numbers of variables grow k-NN algorithm struggles to predict the output of new data point.\n",
    "- k-NN needs homogeneous features: If you decide to build k-NN using a common distance, like Euclidean or Manhattan distances, it is completely necessary that features have the same scale, since absolute differences in features weight the same, i.e., a given distance in feature 1 must means the same for feature 2.\n",
    "- whhat is optimal number of neighbors\n",
    "- imbalanced data causes problems\n",
    "- outlier sensitivity\n",
    "- missing value treatment: k-NN inherently has no capability of dealing with missing value problem."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test k-NN accuracy on test data - 98.8%"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Artificial Neural Network\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Arificial Neural network is quite a big set of algorithms. The main idea is inspired by how our brains works, though it has a lot less in common that we may think.\n",
    "\n",
    "An ANN is based on a collection of connected units or nodes called artificial neurons or perceptron, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit signal to other neurons.\n",
    "\n",
    "There is a Universal approximation theorem[7] that shows, that actually any function can be estimated with a big enough ANN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### General idea"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "'Signal' in a computer is just a float number. The bigger number corresponding to particular neuron, the bigger its activation. There can be many layers of neurons.\n",
    "\n",
    "In a 'classic' Linear Fully conncted network, neurons at each layer are interconnected with all neurons from previous and next layers. Exceptions are input layers (input we get from data), and output layer.\n",
    "\n",
    "Those connections are called edges, and they have some weights (learnable parameters), that explain how different neurons from previos layer impact activation of current neuron from current layer. This simple structure can make network learn difficult pattern\n",
    "\n",
    "\n",
    "Usually one step inside such network can be described in terms of matrix multiplication. Let's say that for some perceptron $i$ on layer $k$ we have vector of weights $w_i$ for each perceptron from previous layer. Then activatin of such neuron can written as a linear combination of activations of all the perceptrons from previous step: $a_i^k=w_{i1} \\cdot a_1^{k-1} + w_{i2} \\cdot a_2^{k-1} + \\cdots w_{in} + \\cdot a_n^{k-1} + b_i=w_i \\cdot a^{k-1} + b_i$\n",
    "\n",
    "Then we can rewrite one layer of activation as a matrix multiplication:\n",
    "\n",
    "$$a^k = W^k \\cdot a^{k-1} + b$$\n",
    "$$W^k=\\begin{bmatrix}\n",
    "-- w_1^k -- \\\\\n",
    "-- w_2^k -- \\\\\n",
    "-- \\cdots -- \\\\\n",
    "-- w_n^k -- \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "-----------\n",
    "\n",
    "\n",
    "<img src=\"./media/nn.jpeg\" alt=\"drawing\" width=\"500\"/>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Activation functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To make ANN learn a non-linear patterns, we have to add so-called 'activation function'. That is some non-linear function `A` that is applied to each neuron at some layer.\n",
    "\n",
    "Ths most popular are:\n",
    "- Sigmoid: $S(a) = \\frac{1}{1+e^{-x}}$\n",
    "- tanh: $T(a) = \\frac{e^{2x} - 1}{e^{2x} + 1}$\n",
    "- Relu: $R(a) = max(0, a)$\n",
    "- Parametruc Relu:\n",
    "\n",
    "\\begin{document}\n",
    "\\[\n",
    "  Pr(a) =\n",
    "  \\begin{cases}\n",
    "    x     & \\text{ if $x > 0$}, \\\\\n",
    "    ax    & \\text{ if $x <= 0$}.\n",
    "  \\end{cases}\n",
    "\\]\n",
    "\\end{document}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Learning ANN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The most popular and most practical way to learn weights and train model - back propagation.\n",
    "\n",
    "That is basically feed data to ANN, compute some loss function (usually defined by the problem we're solving), compute gradients using chain rule, and using some clever modification of gradient descent update all weights in the model. Repeat this process unless value of loss function converges, or we get good enough metrics.\n",
    "\n",
    "Without too much of details, Let's say we have value of loss function $C$. Now since loss function is known, and usually differentiated function is takes as a loss function, we can compute $\\frac{dC}{dA^k}$, where $A^k$ is a output of the model.\n",
    "\n",
    "Then since we know derivative of activation function, we can compute $\\frac{dC}{da^k} = \\frac{dC}{dA^k} \\cdot \\frac{dA^k}{da^k}$, where $a^k$ - perceptrons activations on the last layer. Now since we know that activations are just a linear function of the output of previous step, we can continue this process, until we get to the input layer."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Implementation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In 99% of situations, creating neural network from scratch is not needed, since modern high-level frameworks propose a ton of functionality. Let's use `pytorch` to create a relatively simple fully connected neural network:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class LinearMnistNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 layers_sizes: List | None = None,\n",
    "                 activation=F.relu,\n",
    "                 n_classes=2,\n",
    "                 input_size=784):\n",
    "        super().__init__()\n",
    "        if layers_sizes is None:\n",
    "            layers_sizes = [16, 16]\n",
    "        self.activation = activation\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        prev_size = input_size\n",
    "        for i, cur_size in enumerate(layers_sizes):\n",
    "            self.__setattr__(f\"linear_{i}\", nn.Linear(prev_size, cur_size))\n",
    "            prev_size = cur_size\n",
    "        self.__setattr__(f\"linear_{i + 1}\", nn.Linear(prev_size, n_classes))\n",
    "        self.n_layers = len(layers_sizes) + 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.activation(self.__getattr__(f\"linear_{i}\")(x))\n",
    "        x = F.softmax(x, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MNIST_Dataset(TorchDataset):\n",
    "    def __init__(self,\n",
    "                 dataset: Dataset,\n",
    "                 train=False,\n",
    "                 cv=False,\n",
    "                 test=False,\n",
    "                 length: int | None = None,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        if sum((train, cv, test)) != 1:\n",
    "            raise ValueError(\"Choose only one - train/cv/test\")\n",
    "        if train:\n",
    "            self.data = dataset.train_data[:length]\n",
    "            self.labels = dataset.train_targets[:length]\n",
    "        elif cv:\n",
    "            self.data = dataset.cv_data[:length]\n",
    "            self.labels = dataset.cv_targets[:length]\n",
    "        elif test:\n",
    "            self.data = dataset.test_data[:length]\n",
    "            self.labels = dataset.test_targets[:length]\n",
    "        self.data = torch.from_numpy(self.data.astype(np.float32))\n",
    "        self.labels = torch.from_numpy(self.labels.astype(np.int))\n",
    "        self.length = length if length is not None else len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return torch.flatten(self.data[index]), self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Neural network above can be configured - we can choose number of hidden layers, and their sizes. Also, we can choose an activation function.\n",
    "\n",
    "For a convenience, I also created dataset wrapper, that can work well with a torch.\n",
    "\n",
    "Model training also requires quite a lot of code:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import copy\n",
    "\n",
    "\n",
    "def train_one_epoch(model: torch.nn.Module, optimizer: torch.optim.Optimizer,\n",
    "                    training_loader: DataLoader, loss_fn: Callable,\n",
    "                    print_each: int, verbose=False,\n",
    "                    ) -> List[float]:\n",
    "    running_loss = 0.\n",
    "    loss_history = []\n",
    "    for i, data in enumerate(training_loader):\n",
    "        inputs, labels = data  # get data from batch\n",
    "        optimizer.zero_grad()  # Zero your gradients for every batch\n",
    "        outputs = model(inputs)  # Make predictions for this batch\n",
    "        loss = loss_fn(outputs, labels)  # Compute the loss and its gradients\n",
    "        loss.backward()\n",
    "        optimizer.step()  # Adjust learning weights\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % print_each == print_each - 1:\n",
    "            last_loss = running_loss / print_each  # loss per batch\n",
    "            verbose and print(f\"  batch {i + 1} loss: {last_loss}\")\n",
    "            loss_history.append(float(last_loss))\n",
    "            running_loss = 0.\n",
    "    return loss_history\n",
    "\n",
    "\n",
    "def train_model(\n",
    "        model: torch.nn.Module, optimizer: torch.optim.Optimizer, config: Dict\n",
    ") -> Tuple[Dict[str, List], torch.nn.Module | None]:\n",
    "    deep_cnf = config[\"deep_model\"]\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()  # loss function - cross entropy\n",
    "\n",
    "    train_ds = MNIST_Dataset(dataset, train=True)\n",
    "    cv_ds = MNIST_Dataset(dataset, cv=True)\n",
    "\n",
    "    # prepare data loader\n",
    "    training_loader = DataLoader(\n",
    "        train_ds, batch_size=deep_cnf[\"batch_size\"], shuffle=True, num_workers=deep_cnf[\"loader_num_workers\"]\n",
    "    )\n",
    "    validation_loader = DataLoader(\n",
    "        cv_ds, batch_size=deep_cnf[\"batch_size\"], shuffle=False, num_workers=deep_cnf[\"loader_num_workers\"]\n",
    "    )\n",
    "\n",
    "    print_each = deep_cnf[\"print_each\"]\n",
    "    n_epoch = deep_cnf[\"n_epoch\"]\n",
    "\n",
    "    history = {\n",
    "        \"avg_cv_loss\": [],\n",
    "        \"train_loss\": [],\n",
    "        \"avg_accuracy\": [],\n",
    "        \"avg_f1\": [],\n",
    "    }\n",
    "\n",
    "    best_model = None\n",
    "    best_cv_acc = -1\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        model.train()  # put model in a train mode\n",
    "        loss_h = train_one_epoch(model, optimizer, training_loader, loss_fn, print_each, verbose=False)\n",
    "        model.eval()  # put model in evaluation mode\n",
    "        cv_loss = 0.0\n",
    "        cv_accuracy = 0.0\n",
    "        cv_f1 = 0.0\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            cv_loss += vloss.item()\n",
    "\n",
    "            preds = voutputs.max(dim=1)[1].numpy()\n",
    "            cv_accuracy += accuracy_score(vlabels, preds)\n",
    "            cv_f1 += f1_score(vlabels, preds)\n",
    "\n",
    "        history[\"avg_accuracy\"].append(float(cv_accuracy / (i + 1)))\n",
    "        history[\"avg_f1\"].append(float(cv_loss / (i + 1)))\n",
    "        history[\"avg_cv_loss\"].append(float(cv_f1 / (i + 1)))\n",
    "        history[\"train_loss\"] += loss_h\n",
    "\n",
    "        avg_train_loss = np.mean(loss_h)\n",
    "        print(f\"EPOCH: {epoch} done. loss: {avg_train_loss}, accuracy: {history['avg_accuracy'][-1]}\")\n",
    "\n",
    "        # update best model\n",
    "        if history['avg_accuracy'][-1] > best_cv_acc:\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_cv_acc = history['avg_accuracy'][-1]\n",
    "\n",
    "    return history, best_model\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also, we will need some functions to evaluate model on the test dataset, vizualize training history:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "def vizualize_history(history: Dict):\n",
    "    fig, axes = plt.subplots(3, 1, sharex=True)\n",
    "    fig_width, fig_height = fig.get_size_inches()\n",
    "    fig.set_size_inches(fig_width, fig_height * 3)\n",
    "    idx = 0\n",
    "    for key in history:\n",
    "        if key == \"train_loss\":\n",
    "            continue\n",
    "        axes[idx].set_title(key)\n",
    "        axes[idx].set_xlabel(\"epoch\")\n",
    "        axes[idx].plot(history[key])\n",
    "        idx += 1\n",
    "    plt.show()\n",
    "    plt.title(\"train loss\")\n",
    "    plt.plot(history[\"train_loss\"])\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def eval_model(model: Module) -> Dict:\n",
    "    model.eval()\n",
    "\n",
    "    inputs = torch.from_numpy(dataset.test_data.astype(np.float32))\n",
    "    labels = dataset.test_targets\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    preds = outputs.max(dim=1)[1].numpy()\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "\n",
    "    print(f\"Test accuracy: {acc}, f1-score: {f1}\")\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1-score\": f1,\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19987/1468713152.py:57: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.labels = torch.from_numpy(self.labels.astype(np.int))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 done. loss: 0.36828688029889706, accuracy: 0.9559859154929577\n",
      "EPOCH: 1 done. loss: 0.3439179410537084, accuracy: 0.9709507042253521\n",
      "EPOCH: 2 done. loss: 0.3376657319289667, accuracy: 0.9762323943661971\n",
      "EPOCH: 3 done. loss: 0.33580154798648976, accuracy: 0.9815140845070423\n",
      "EPOCH: 4 done. loss: 0.3316431481529165, accuracy: 0.9779929577464789\n",
      "EPOCH: 5 done. loss: 0.3299776365359624, accuracy: 0.9810739436619719\n",
      "EPOCH: 6 done. loss: 0.33410466059490485, accuracy: 0.9797535211267606\n",
      "EPOCH: 7 done. loss: 0.33140984332119977, accuracy: 0.9775528169014085\n",
      "EPOCH: 8 done. loss: 0.3306962721877628, accuracy: 0.9841549295774648\n",
      "EPOCH: 9 done. loss: 0.3322595906478387, accuracy: 0.9797535211267606\n",
      "EPOCH: 10 done. loss: 0.3268698588565544, accuracy: 0.9665492957746479\n",
      "EPOCH: 11 done. loss: 0.33122588760322996, accuracy: 0.9766725352112676\n",
      "EPOCH: 12 done. loss: 0.32847178114785086, accuracy: 0.983274647887324\n",
      "EPOCH: 13 done. loss: 0.32821037250536456, accuracy: 0.9793133802816901\n",
      "EPOCH: 14 done. loss: 0.32682477600044674, accuracy: 0.9722711267605634\n",
      "EPOCH: 15 done. loss: 0.3346402529213164, accuracy: 0.980193661971831\n",
      "EPOCH: 16 done. loss: 0.33034214973449705, accuracy: 0.9797535211267606\n",
      "EPOCH: 17 done. loss: 0.32882354049770923, accuracy: 0.9753521126760564\n",
      "EPOCH: 18 done. loss: 0.33082158135043255, accuracy: 0.980193661971831\n",
      "EPOCH: 19 done. loss: 0.3292068122713654, accuracy: 0.9766725352112676\n",
      "EPOCH: 20 done. loss: 0.331015752531864, accuracy: 0.9779929577464789\n",
      "EPOCH: 21 done. loss: 0.331569351973357, accuracy: 0.9779929577464789\n",
      "EPOCH: 22 done. loss: 0.32793594786414393, accuracy: 0.9797535211267606\n",
      "EPOCH: 23 done. loss: 0.32953280652010886, accuracy: 0.9806338028169014\n",
      "EPOCH: 24 done. loss: 0.3281234630831966, accuracy: 0.9546654929577465\n",
      "EPOCH: 25 done. loss: 0.3342933301572446, accuracy: 0.9819542253521126\n",
      "EPOCH: 26 done. loss: 0.3318661242723465, accuracy: 0.983274647887324\n",
      "EPOCH: 27 done. loss: 0.32918111692976065, accuracy: 0.9775528169014085\n",
      "EPOCH: 28 done. loss: 0.3302964756886164, accuracy: 0.9775528169014085\n",
      "EPOCH: 29 done. loss: 0.3318899563065282, accuracy: 0.9823943661971831\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x864 with 3 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9866023579849946, f1-score: 0.987120041215868\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'accuracy': 0.9866023579849946, 'f1-score': 0.987120041215868}"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "config = {\n",
    "    \"deep_model\": {\n",
    "        \"linear\": {\n",
    "            \"layers_sizes\": [16, 16],\n",
    "            \"activation\": \"relu\",\n",
    "        },\n",
    "        \"batch_size\": 32,\n",
    "        \"loader_num_workers\": 4,\n",
    "        \"print_each\": 30,\n",
    "        \"n_epoch\": 30,\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "model = LinearMnistNN()\n",
    "optimizer = Adam(list(model.parameters()), lr=0.005, weight_decay=0.001)\n",
    "history, best_model = train_model(model, optimizer, config)\n",
    "vizualize_history(history)\n",
    "\n",
    "eval_model(best_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's try to fine tune the model:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def grid_search_nn(\n",
    "        layers: List[List[int]],\n",
    "        optimizers: List[Tuple[type, Dict[str, float]]],\n",
    "        activations: List[Callable],\n",
    "        config: Dict,\n",
    "):\n",
    "    very_best_model = None\n",
    "    very_best_val_accuracy = -1\n",
    "    very_best_config = {}\n",
    "    results = []\n",
    "\n",
    "    for layer_sizes in layers:\n",
    "        print(f\"Current layers: {layer_sizes}\")\n",
    "        for opt_cnf in optimizers:\n",
    "            opt_cnf_full = copy.deepcopy(opt_cnf[1])\n",
    "            opt_cnf_full.update({\"name\": opt_cnf[0].__name__})\n",
    "            print(f\"  Current optimizer: {opt_cnf_full}\")\n",
    "            for activation in activations:\n",
    "                print(f\"    Current activation: {activation.__name__}\")\n",
    "                model = LinearMnistNN(layer_sizes, activation)\n",
    "                optimizer = opt_cnf[0](list(model.parameters()), **opt_cnf[1])\n",
    "\n",
    "                history, best_model = train_model(model, optimizer, config)\n",
    "\n",
    "                cur_best_cv_acc = min(history[\"avg_cv_loss\"])\n",
    "                # cur_best_cv_f1 = min(history[\"avg_f1\"])\n",
    "\n",
    "                cur_configuration = {\n",
    "                    \"layers\": layer_sizes,\n",
    "                    \"activation\": activation.__name__,\n",
    "                    \"optimizer_cnf\": opt_cnf_full,\n",
    "                    \"cv_accuracy\": cur_best_cv_acc,\n",
    "                }\n",
    "\n",
    "                if cur_best_cv_acc > very_best_val_accuracy:\n",
    "                    very_best_model = best_model\n",
    "                    very_best_config = cur_configuration\n",
    "\n",
    "                results.append(cur_configuration)\n",
    "\n",
    "    print(f\"best model has accuracy {very_best_val_accuracy}, and config:\")\n",
    "    pprint(very_best_config)\n",
    "    return results, very_best_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current layers: [16, 16]\n",
      "  Current optimizer: {'lr': 0.005, 'weight_decay': 0.005, 'name': 'Adam'}\n",
      "    Current activation: relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19987/1468713152.py:57: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.labels = torch.from_numpy(self.labels.astype(np.int))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 done. loss: 0.5510607776818452, accuracy: 0.9432218309859155\n",
      "EPOCH: 1 done. loss: 0.3540600225881294, accuracy: 0.9744718309859155\n",
      "EPOCH: 2 done. loss: 0.3424782419646228, accuracy: 0.9705105633802817\n",
      "EPOCH: 3 done. loss: 0.34266777921605995, accuracy: 0.9762323943661971\n",
      "EPOCH: 4 done. loss: 0.34035911515907, accuracy: 0.9753521126760564\n",
      "EPOCH: 5 done. loss: 0.3411540510477843, accuracy: 0.9757922535211268\n",
      "EPOCH: 6 done. loss: 0.33798194008844873, accuracy: 0.9700704225352113\n",
      "EPOCH: 7 done. loss: 0.3401106388480575, accuracy: 0.9678697183098591\n",
      "EPOCH: 8 done. loss: 0.3417808498497363, accuracy: 0.9753521126760564\n",
      "EPOCH: 9 done. loss: 0.33824292840781034, accuracy: 0.9507042253521126\n",
      "EPOCH: 10 done. loss: 0.3396172206710886, accuracy: 0.9705105633802817\n",
      "EPOCH: 11 done. loss: 0.3381507812826722, accuracy: 0.9727112676056338\n",
      "EPOCH: 12 done. loss: 0.33970494711840593, accuracy: 0.977112676056338\n",
      "EPOCH: 13 done. loss: 0.33963172270192044, accuracy: 0.9647887323943662\n",
      "EPOCH: 14 done. loss: 0.33684574994775984, accuracy: 0.9744718309859155\n",
      "EPOCH: 15 done. loss: 0.33934304526558623, accuracy: 0.9621478873239436\n",
      "EPOCH: 16 done. loss: 0.3394867381563893, accuracy: 0.977112676056338\n",
      "EPOCH: 17 done. loss: 0.3371855483010963, accuracy: 0.971830985915493\n",
      "EPOCH: 18 done. loss: 0.3385951079704143, accuracy: 0.9753521126760564\n",
      "EPOCH: 19 done. loss: 0.3388946939397741, accuracy: 0.9735915492957746\n",
      "EPOCH: 20 done. loss: 0.339108673841865, accuracy: 0.9735915492957746\n",
      "EPOCH: 21 done. loss: 0.33877366284529364, accuracy: 0.9700704225352113\n",
      "EPOCH: 22 done. loss: 0.33886724655274997, accuracy: 0.9757922535211268\n",
      "EPOCH: 23 done. loss: 0.3403490541157899, accuracy: 0.9577464788732394\n",
      "EPOCH: 24 done. loss: 0.33927544035293433, accuracy: 0.9735915492957746\n",
      "EPOCH: 25 done. loss: 0.3367466432076914, accuracy: 0.9691901408450704\n",
      "EPOCH: 26 done. loss: 0.33964642319414357, accuracy: 0.9722711267605634\n",
      "EPOCH: 27 done. loss: 0.33990810623875367, accuracy: 0.9700704225352113\n",
      "EPOCH: 28 done. loss: 0.3384943544864655, accuracy: 0.9735915492957746\n",
      "EPOCH: 29 done. loss: 0.3367328855726454, accuracy: 0.9762323943661971\n",
      "    Current activation: sigmoid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19987/1468713152.py:57: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.labels = torch.from_numpy(self.labels.astype(np.int))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 done. loss: 0.5783932277449854, accuracy: 0.9379401408450704\n",
      "EPOCH: 1 done. loss: 0.5407085182490172, accuracy: 0.94712441314554\n",
      "EPOCH: 2 done. loss: 0.5418135859348155, accuracy: 0.9480633802816901\n",
      "EPOCH: 3 done. loss: 0.5407055896741373, accuracy: 0.9436619718309859\n",
      "EPOCH: 4 done. loss: 0.5399503961757376, accuracy: 0.9454225352112676\n",
      "EPOCH: 5 done. loss: 0.5411756875338377, accuracy: 0.9419014084507042\n",
      "EPOCH: 6 done. loss: 0.5433107826444838, accuracy: 0.9480633802816901\n",
      "EPOCH: 7 done. loss: 0.5397484638072826, accuracy: 0.9463028169014085\n",
      "EPOCH: 8 done. loss: 0.5414109088756419, accuracy: 0.9471830985915493\n",
      "EPOCH: 9 done. loss: 0.539328975809945, accuracy: 0.9449823943661971\n",
      "EPOCH: 10 done. loss: 0.5407374322414399, accuracy: 0.9502640845070423\n",
      "EPOCH: 11 done. loss: 0.5423716370706205, accuracy: 0.9445422535211268\n",
      "EPOCH: 12 done. loss: 0.5403409132251032, accuracy: 0.9352992957746479\n",
      "EPOCH: 13 done. loss: 0.5404019607437981, accuracy: 0.9485035211267606\n",
      "EPOCH: 14 done. loss: 0.5399782706190039, accuracy: 0.9511443661971831\n",
      "EPOCH: 15 done. loss: 0.5398666752709282, accuracy: 0.9476232394366197\n",
      "EPOCH: 16 done. loss: 0.5414139114044331, accuracy: 0.9449823943661971\n",
      "EPOCH: 17 done. loss: 0.5420791537673386, accuracy: 0.9502640845070423\n",
      "EPOCH: 18 done. loss: 0.5402648148713288, accuracy: 0.929137323943662\n",
      "EPOCH: 19 done. loss: 0.5407416471728571, accuracy: 0.9383802816901409\n",
      "EPOCH: 20 done. loss: 0.5398259602211141, accuracy: 0.9432218309859155\n",
      "EPOCH: 21 done. loss: 0.5410407521106579, accuracy: 0.9348591549295775\n",
      "EPOCH: 22 done. loss: 0.5402587298993711, accuracy: 0.9361795774647887\n",
      "EPOCH: 23 done. loss: 0.5399844600094689, accuracy: 0.9383802816901409\n",
      "EPOCH: 24 done. loss: 0.5403278648853302, accuracy: 0.9427816901408451\n",
      "EPOCH: 25 done. loss: 0.5412721062148058, accuracy: 0.9423415492957746\n",
      "EPOCH: 26 done. loss: 0.5416902233053136, accuracy: 0.9467429577464789\n",
      "EPOCH: 27 done. loss: 0.5411801225609251, accuracy: 0.9388204225352113\n",
      "EPOCH: 28 done. loss: 0.5397220417305275, accuracy: 0.9419014084507042\n",
      "EPOCH: 29 done. loss: 0.54077806185793, accuracy: 0.9427816901408451\n",
      "  Current optimizer: {'lr': 0.0005, 'weight_decay': 0.001, 'name': 'Adam'}\n",
      "    Current activation: relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19987/1468713152.py:57: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.labels = torch.from_numpy(self.labels.astype(np.int))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 done. loss: 0.41952837275134197, accuracy: 0.9617077464788732\n",
      "EPOCH: 1 done. loss: 0.342002793263506, accuracy: 0.9762323943661971\n",
      "EPOCH: 2 done. loss: 0.3341100031578982, accuracy: 0.980193661971831\n",
      "EPOCH: 3 done. loss: 0.3303932032099477, accuracy: 0.980193661971831\n",
      "EPOCH: 4 done. loss: 0.32607849074734585, accuracy: 0.9793133802816901\n",
      "EPOCH: 5 done. loss: 0.3248033609655169, accuracy: 0.983274647887324\n",
      "EPOCH: 6 done. loss: 0.32381907326203807, accuracy: 0.983274647887324\n",
      "EPOCH: 7 done. loss: 0.32208533772715814, accuracy: 0.983274647887324\n",
      "EPOCH: 8 done. loss: 0.3215373966428968, accuracy: 0.9837147887323944\n",
      "EPOCH: 9 done. loss: 0.3211740914318297, accuracy: 0.9850352112676056\n",
      "EPOCH: 10 done. loss: 0.3201663832973551, accuracy: 0.9837147887323944\n",
      "EPOCH: 11 done. loss: 0.3197556801416256, accuracy: 0.9859154929577465\n",
      "EPOCH: 12 done. loss: 0.3198327232290197, accuracy: 0.9841549295774648\n",
      "EPOCH: 13 done. loss: 0.3190106730770182, accuracy: 0.9867957746478874\n",
      "EPOCH: 14 done. loss: 0.31856386175862067, accuracy: 0.9859154929577465\n",
      "EPOCH: 15 done. loss: 0.31864020460181763, accuracy: 0.9859154929577465\n",
      "EPOCH: 16 done. loss: 0.319531504313151, accuracy: 0.9859154929577465\n",
      "EPOCH: 17 done. loss: 0.3183032626355136, accuracy: 0.9841549295774648\n",
      "EPOCH: 18 done. loss: 0.3185040872406077, accuracy: 0.9863556338028169\n",
      "EPOCH: 19 done. loss: 0.31761943256413494, accuracy: 0.9863556338028169\n",
      "EPOCH: 20 done. loss: 0.3172016361245403, accuracy: 0.9863556338028169\n",
      "EPOCH: 21 done. loss: 0.31715309465373004, accuracy: 0.985475352112676\n",
      "EPOCH: 22 done. loss: 0.318541854068085, accuracy: 0.985475352112676\n",
      "EPOCH: 23 done. loss: 0.31775702414689244, accuracy: 0.9845950704225352\n",
      "EPOCH: 24 done. loss: 0.3168239069205743, accuracy: 0.985475352112676\n",
      "EPOCH: 25 done. loss: 0.3167111433214611, accuracy: 0.9863556338028169\n",
      "EPOCH: 26 done. loss: 0.3164379347253729, accuracy: 0.9872359154929577\n",
      "EPOCH: 27 done. loss: 0.316639651523696, accuracy: 0.9876760563380281\n",
      "EPOCH: 28 done. loss: 0.31699472489180386, accuracy: 0.988556338028169\n",
      "EPOCH: 29 done. loss: 0.31594147693227836, accuracy: 0.9876760563380281\n",
      "    Current activation: sigmoid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19987/1468713152.py:57: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.labels = torch.from_numpy(self.labels.astype(np.int))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 done. loss: 0.6624051308190382, accuracy: 0.9432218309859155\n",
      "EPOCH: 1 done. loss: 0.572910616132948, accuracy: 0.952024647887324\n",
      "EPOCH: 2 done. loss: 0.5369319138703523, accuracy: 0.9546654929577465\n",
      "EPOCH: 3 done. loss: 0.5259237987023813, accuracy: 0.9555457746478874\n",
      "EPOCH: 4 done. loss: 0.5206078542603386, accuracy: 0.9577464788732394\n",
      "EPOCH: 5 done. loss: 0.5178186086592852, accuracy: 0.9581866197183099\n",
      "EPOCH: 6 done. loss: 0.515693144776203, accuracy: 0.9586267605633803\n",
      "EPOCH: 7 done. loss: 0.514922488729159, accuracy: 0.9595070422535211\n",
      "EPOCH: 8 done. loss: 0.513487653930982, accuracy: 0.9568661971830986\n",
      "EPOCH: 9 done. loss: 0.5129345445721237, accuracy: 0.9595070422535211\n",
      "EPOCH: 10 done. loss: 0.5125202959334408, accuracy: 0.9590669014084507\n",
      "EPOCH: 11 done. loss: 0.5118050833543142, accuracy: 0.9612676056338029\n",
      "EPOCH: 12 done. loss: 0.511322315202819, accuracy: 0.9577464788732394\n",
      "EPOCH: 13 done. loss: 0.5110948400364982, accuracy: 0.9608274647887324\n",
      "EPOCH: 14 done. loss: 0.5111779891782336, accuracy: 0.9590669014084507\n",
      "EPOCH: 15 done. loss: 0.5108285776994846, accuracy: 0.9590669014084507\n",
      "EPOCH: 16 done. loss: 0.5105750835604138, accuracy: 0.957306338028169\n",
      "EPOCH: 17 done. loss: 0.5105970132130163, accuracy: 0.9586267605633803\n",
      "EPOCH: 18 done. loss: 0.5105170355902778, accuracy: 0.9577464788732394\n",
      "EPOCH: 19 done. loss: 0.5103571783613275, accuracy: 0.960387323943662\n",
      "EPOCH: 20 done. loss: 0.5104339585260109, accuracy: 0.9590669014084507\n",
      "EPOCH: 21 done. loss: 0.5105230887730917, accuracy: 0.960387323943662\n",
      "EPOCH: 22 done. loss: 0.5103705905101916, accuracy: 0.9581866197183099\n",
      "EPOCH: 23 done. loss: 0.510481791915717, accuracy: 0.9595070422535211\n",
      "EPOCH: 24 done. loss: 0.5101869618451155, accuracy: 0.9568661971830986\n",
      "EPOCH: 25 done. loss: 0.5102804561456045, accuracy: 0.9581866197183099\n",
      "EPOCH: 26 done. loss: 0.5103283481465446, accuracy: 0.9586267605633803\n",
      "EPOCH: 27 done. loss: 0.5101817583596264, accuracy: 0.9555457746478874\n",
      "EPOCH: 28 done. loss: 0.5106260801906939, accuracy: 0.9599471830985915\n",
      "EPOCH: 29 done. loss: 0.5104594547439505, accuracy: 0.9590669014084507\n",
      "  Current optimizer: {'lr': 0.005, 'weight_decay': 0.005, 'name': 'SGD'}\n",
      "    Current activation: relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19987/1468713152.py:57: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.labels = torch.from_numpy(self.labels.astype(np.int))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 done. loss: 0.6932664155960082, accuracy: 0.49392605633802816\n",
      "EPOCH: 1 done. loss: 0.691686213237268, accuracy: 0.49392605633802816\n",
      "EPOCH: 2 done. loss: 0.661477428233182, accuracy: 0.49392605633802816\n",
      "EPOCH: 3 done. loss: 0.6050229748090108, accuracy: 0.49392605633802816\n",
      "EPOCH: 4 done. loss: 0.5716962330871158, accuracy: 0.49392605633802816\n",
      "EPOCH: 5 done. loss: 0.5568639978214547, accuracy: 0.49392605633802816\n",
      "EPOCH: 6 done. loss: 0.5475138749237414, accuracy: 0.49392605633802816\n",
      "EPOCH: 7 done. loss: 0.5430721098626101, accuracy: 0.49392605633802816\n",
      "EPOCH: 8 done. loss: 0.5392961295666518, accuracy: 0.49392605633802816\n",
      "EPOCH: 9 done. loss: 0.536611732950917, accuracy: 0.49392605633802816\n",
      "EPOCH: 10 done. loss: 0.5356489216839826, accuracy: 0.49392605633802816\n",
      "EPOCH: 11 done. loss: 0.5336785533913859, accuracy: 0.49392605633802816\n",
      "EPOCH: 12 done. loss: 0.5313441911229382, accuracy: 0.49392605633802816\n",
      "EPOCH: 13 done. loss: 0.5316579454474979, accuracy: 0.49392605633802816\n",
      "EPOCH: 14 done. loss: 0.5309278924156118, accuracy: 0.49392605633802816\n",
      "EPOCH: 15 done. loss: 0.5297429183015117, accuracy: 0.49392605633802816\n",
      "EPOCH: 16 done. loss: 0.5287280582719379, accuracy: 0.49392605633802816\n",
      "EPOCH: 17 done. loss: 0.5277857235184422, accuracy: 0.49392605633802816\n",
      "EPOCH: 18 done. loss: 0.5280586240468202, accuracy: 0.49392605633802816\n",
      "EPOCH: 19 done. loss: 0.5275211818792201, accuracy: 0.49392605633802816\n",
      "EPOCH: 20 done. loss: 0.5273657104483357, accuracy: 0.49392605633802816\n",
      "EPOCH: 21 done. loss: 0.5264714651637608, accuracy: 0.49392605633802816\n",
      "EPOCH: 22 done. loss: 0.5266153693199158, accuracy: 0.49392605633802816\n",
      "EPOCH: 23 done. loss: 0.5255472664479856, accuracy: 0.49392605633802816\n",
      "EPOCH: 24 done. loss: 0.5257878612588953, accuracy: 0.49392605633802816\n",
      "EPOCH: 25 done. loss: 0.5250977861660497, accuracy: 0.49392605633802816\n",
      "EPOCH: 26 done. loss: 0.5246389700306786, accuracy: 0.49392605633802816\n",
      "EPOCH: 27 done. loss: 0.5234976630519939, accuracy: 0.49392605633802816\n",
      "EPOCH: 28 done. loss: 0.5249102752517771, accuracy: 0.49392605633802816\n",
      "EPOCH: 29 done. loss: 0.5238323897123336, accuracy: 0.49392605633802816\n",
      "    Current activation: sigmoid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19987/1468713152.py:57: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.labels = torch.from_numpy(self.labels.astype(np.int))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 done. loss: 0.6942905355382849, accuracy: 0.49392605633802816\n",
      "EPOCH: 1 done. loss: 0.6939570398242386, accuracy: 0.49392605633802816\n",
      "EPOCH: 2 done. loss: 0.69355497735518, accuracy: 0.49392605633802816\n",
      "EPOCH: 3 done. loss: 0.6932803312937419, accuracy: 0.49392605633802816\n",
      "EPOCH: 4 done. loss: 0.6930572127854383, accuracy: 0.49392605633802816\n",
      "EPOCH: 5 done. loss: 0.6928289192694205, accuracy: 0.49392605633802816\n",
      "EPOCH: 6 done. loss: 0.6926026975667035, accuracy: 0.5661091549295775\n",
      "EPOCH: 7 done. loss: 0.6924090798254368, accuracy: 0.6584213615023474\n",
      "EPOCH: 8 done. loss: 0.6922218556757327, accuracy: 0.5060739436619718\n",
      "EPOCH: 9 done. loss: 0.6920669365812231, accuracy: 0.5060739436619718\n",
      "EPOCH: 10 done. loss: 0.6919083593068299, accuracy: 0.5060739436619718\n",
      "EPOCH: 11 done. loss: 0.6917710922382496, accuracy: 0.5060739436619718\n",
      "EPOCH: 12 done. loss: 0.691613135955952, accuracy: 0.5060739436619718\n",
      "EPOCH: 13 done. loss: 0.6915209545029533, accuracy: 0.5060739436619718\n",
      "EPOCH: 14 done. loss: 0.691380708968198, accuracy: 0.5060739436619718\n",
      "EPOCH: 15 done. loss: 0.6912252434977779, accuracy: 0.5060739436619718\n",
      "EPOCH: 16 done. loss: 0.6911617241523884, accuracy: 0.5060739436619718\n",
      "EPOCH: 17 done. loss: 0.690965575421298, accuracy: 0.5060739436619718\n",
      "EPOCH: 18 done. loss: 0.6909493157157193, accuracy: 0.5060739436619718\n",
      "EPOCH: 19 done. loss: 0.6908148741280591, accuracy: 0.5060739436619718\n",
      "EPOCH: 20 done. loss: 0.6907765236165788, accuracy: 0.5060739436619718\n",
      "EPOCH: 21 done. loss: 0.6906477129017865, accuracy: 0.5060739436619718\n",
      "EPOCH: 22 done. loss: 0.6906036065684424, accuracy: 0.5060739436619718\n",
      "EPOCH: 23 done. loss: 0.6903136429963288, accuracy: 0.5060739436619718\n",
      "EPOCH: 24 done. loss: 0.6903948011221708, accuracy: 0.5060739436619718\n",
      "EPOCH: 25 done. loss: 0.6903027715506377, accuracy: 0.5060739436619718\n",
      "EPOCH: 26 done. loss: 0.6901625472086448, accuracy: 0.5060739436619718\n",
      "EPOCH: 27 done. loss: 0.6900878367600618, accuracy: 0.5060739436619718\n",
      "EPOCH: 28 done. loss: 0.6900164794038843, accuracy: 0.5060739436619718\n",
      "EPOCH: 29 done. loss: 0.6899205168088277, accuracy: 0.5060739436619718\n",
      "  Current optimizer: {'lr': 0.0005, 'weight_decay': 0.001, 'name': 'SGD'}\n",
      "    Current activation: relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19987/1468713152.py:57: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.labels = torch.from_numpy(self.labels.astype(np.int))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 done. loss: 0.6913865508856598, accuracy: 0.49392605633802816\n",
      "EPOCH: 1 done. loss: 0.6852417976767928, accuracy: 0.49392605633802816\n",
      "EPOCH: 2 done. loss: 0.6784467818560423, accuracy: 0.49392605633802816\n",
      "EPOCH: 3 done. loss: 0.6712134665913051, accuracy: 0.49392605633802816\n",
      "EPOCH: 4 done. loss: 0.6633627834143462, accuracy: 0.49392605633802816\n",
      "EPOCH: 5 done. loss: 0.6562238209777408, accuracy: 0.49392605633802816\n",
      "EPOCH: 6 done. loss: 0.6486880580584208, accuracy: 0.49392605633802816\n",
      "EPOCH: 7 done. loss: 0.6413980378044976, accuracy: 0.49392605633802816\n",
      "EPOCH: 8 done. loss: 0.6350936324508102, accuracy: 0.49392605633802816\n",
      "EPOCH: 9 done. loss: 0.6284042338530224, accuracy: 0.49392605633802816\n",
      "EPOCH: 10 done. loss: 0.6219355503718058, accuracy: 0.49392605633802816\n",
      "EPOCH: 11 done. loss: 0.6159790133988415, accuracy: 0.49392605633802816\n",
      "EPOCH: 12 done. loss: 0.6102859154895499, accuracy: 0.49392605633802816\n",
      "EPOCH: 13 done. loss: 0.6055026524596745, accuracy: 0.49392605633802816\n",
      "EPOCH: 14 done. loss: 0.6005848621880567, accuracy: 0.49392605633802816\n",
      "EPOCH: 15 done. loss: 0.5955087617591575, accuracy: 0.49392605633802816\n",
      "EPOCH: 16 done. loss: 0.5921842831152456, accuracy: 0.49392605633802816\n",
      "EPOCH: 17 done. loss: 0.5883661062629134, accuracy: 0.49392605633802816\n",
      "EPOCH: 18 done. loss: 0.5850504819993619, accuracy: 0.49392605633802816\n",
      "EPOCH: 19 done. loss: 0.5822813016396982, accuracy: 0.49392605633802816\n",
      "EPOCH: 20 done. loss: 0.5797061576887415, accuracy: 0.49392605633802816\n",
      "EPOCH: 21 done. loss: 0.5766980212043833, accuracy: 0.49392605633802816\n",
      "EPOCH: 22 done. loss: 0.5746487352583143, accuracy: 0.49392605633802816\n",
      "EPOCH: 23 done. loss: 0.5725520037942462, accuracy: 0.49392605633802816\n",
      "EPOCH: 24 done. loss: 0.5710152969316201, accuracy: 0.49392605633802816\n",
      "EPOCH: 25 done. loss: 0.5695224903247974, accuracy: 0.49392605633802816\n",
      "EPOCH: 26 done. loss: 0.567249756389194, accuracy: 0.49392605633802816\n",
      "EPOCH: 27 done. loss: 0.5660147696733474, accuracy: 0.49392605633802816\n",
      "EPOCH: 28 done. loss: 0.5642715755436156, accuracy: 0.49392605633802816\n",
      "EPOCH: 29 done. loss: 0.5624057365788354, accuracy: 0.49392605633802816\n",
      "    Current activation: sigmoid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19987/1468713152.py:57: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.labels = torch.from_numpy(self.labels.astype(np.int))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 done. loss: 0.6926467345820533, accuracy: 0.5060739436619718\n",
      "EPOCH: 1 done. loss: 0.6926061257168098, accuracy: 0.5060739436619718\n",
      "EPOCH: 2 done. loss: 0.6926201809335638, accuracy: 0.5060739436619718\n",
      "EPOCH: 3 done. loss: 0.6926006785145512, accuracy: 0.5060739436619718\n",
      "EPOCH: 4 done. loss: 0.6925570602770205, accuracy: 0.5060739436619718\n",
      "EPOCH: 5 done. loss: 0.6925411838072317, accuracy: 0.5060739436619718\n",
      "EPOCH: 6 done. loss: 0.6925272305806478, accuracy: 0.5060739436619718\n",
      "EPOCH: 7 done. loss: 0.6925502258318442, accuracy: 0.5060739436619718\n",
      "EPOCH: 8 done. loss: 0.6924967785676321, accuracy: 0.5060739436619718\n",
      "EPOCH: 9 done. loss: 0.6924501204932176, accuracy: 0.5060739436619718\n",
      "EPOCH: 10 done. loss: 0.6924516293737625, accuracy: 0.5060739436619718\n",
      "EPOCH: 11 done. loss: 0.6924554617316634, accuracy: 0.5060739436619718\n",
      "EPOCH: 12 done. loss: 0.6924765251300954, accuracy: 0.5060739436619718\n",
      "EPOCH: 13 done. loss: 0.6923744053752334, accuracy: 0.5060739436619718\n",
      "EPOCH: 14 done. loss: 0.6923562690063759, accuracy: 0.5060739436619718\n",
      "EPOCH: 15 done. loss: 0.69234936259411, accuracy: 0.5060739436619718\n",
      "EPOCH: 16 done. loss: 0.6923485192987654, accuracy: 0.5060739436619718\n",
      "EPOCH: 17 done. loss: 0.6922603377589472, accuracy: 0.5060739436619718\n",
      "EPOCH: 18 done. loss: 0.692282130100109, accuracy: 0.5060739436619718\n",
      "EPOCH: 19 done. loss: 0.6922998642479932, accuracy: 0.5060739436619718\n",
      "EPOCH: 20 done. loss: 0.6923238796216471, accuracy: 0.5060739436619718\n",
      "EPOCH: 21 done. loss: 0.692253084535952, accuracy: 0.5060739436619718\n",
      "EPOCH: 22 done. loss: 0.6922399569440771, accuracy: 0.5060739436619718\n",
      "EPOCH: 23 done. loss: 0.6922826941366549, accuracy: 0.5060739436619718\n",
      "EPOCH: 24 done. loss: 0.6921958985152068, accuracy: 0.5060739436619718\n",
      "EPOCH: 25 done. loss: 0.6921847725356066, accuracy: 0.5060739436619718\n",
      "EPOCH: 26 done. loss: 0.692234844410861, accuracy: 0.5060739436619718\n",
      "EPOCH: 27 done. loss: 0.6921317133638595, accuracy: 0.5060739436619718\n",
      "EPOCH: 28 done. loss: 0.6921437283356985, accuracy: 0.5060739436619718\n",
      "EPOCH: 29 done. loss: 0.6921323317068595, accuracy: 0.5060739436619718\n",
      "Current layers: [64, 32, 16]\n",
      "  Current optimizer: {'lr': 0.005, 'weight_decay': 0.005, 'name': 'Adam'}\n",
      "    Current activation: relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19987/1468713152.py:57: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.labels = torch.from_numpy(self.labels.astype(np.int))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 done. loss: 0.3744941329514539, accuracy: 0.9532863849765258\n",
      "EPOCH: 1 done. loss: 0.3499820309656638, accuracy: 0.9709507042253521\n",
      "EPOCH: 2 done. loss: 0.34989318008776066, accuracy: 0.9713908450704225\n",
      "EPOCH: 3 done. loss: 0.34332800220560145, accuracy: 0.9639084507042254\n",
      "EPOCH: 4 done. loss: 0.34632248525266296, accuracy: 0.9674295774647887\n",
      "EPOCH: 5 done. loss: 0.34123147339732557, accuracy: 0.9617077464788732\n",
      "EPOCH: 6 done. loss: 0.34351955232796844, accuracy: 0.9731514084507042\n",
      "EPOCH: 7 done. loss: 0.3405587634554616, accuracy: 0.9775528169014085\n",
      "EPOCH: 8 done. loss: 0.3442628957607128, accuracy: 0.9674295774647887\n",
      "EPOCH: 9 done. loss: 0.34102684634703184, accuracy: 0.9749119718309859\n",
      "EPOCH: 10 done. loss: 0.3396371325960865, accuracy: 0.9713908450704225\n",
      "EPOCH: 11 done. loss: 0.3395750458593722, accuracy: 0.9727112676056338\n",
      "EPOCH: 12 done. loss: 0.34349905682934656, accuracy: 0.9727112676056338\n",
      "EPOCH: 13 done. loss: 0.33805951133922296, accuracy: 0.9577464788732394\n",
      "EPOCH: 14 done. loss: 0.34121456709172987, accuracy: 0.9608274647887324\n",
      "EPOCH: 15 done. loss: 0.3414240573291425, accuracy: 0.9568661971830986\n",
      "EPOCH: 16 done. loss: 0.33901609899821106, accuracy: 0.9639084507042254\n",
      "EPOCH: 17 done. loss: 0.3401745167043474, accuracy: 0.9727112676056338\n",
      "EPOCH: 18 done. loss: 0.3390865138283482, accuracy: 0.9678697183098591\n",
      "EPOCH: 19 done. loss: 0.33928505767274786, accuracy: 0.9731514084507042\n",
      "EPOCH: 20 done. loss: 0.3359285638288216, accuracy: 0.9665492957746479\n",
      "EPOCH: 21 done. loss: 0.3391705924714053, accuracy: 0.9674295774647887\n",
      "EPOCH: 22 done. loss: 0.3357093111232474, accuracy: 0.971830985915493\n",
      "EPOCH: 23 done. loss: 0.3391537677358698, accuracy: 0.9779929577464789\n",
      "EPOCH: 24 done. loss: 0.34101789284635475, accuracy: 0.9757922535211268\n",
      "EPOCH: 25 done. loss: 0.3372582079083831, accuracy: 0.9608274647887324\n",
      "EPOCH: 26 done. loss: 0.34033495066342534, accuracy: 0.9674295774647887\n",
      "EPOCH: 27 done. loss: 0.3403755248696716, accuracy: 0.954225352112676\n",
      "EPOCH: 28 done. loss: 0.33882692919837104, accuracy: 0.9797535211267606\n",
      "EPOCH: 29 done. loss: 0.3385280422590397, accuracy: 0.9595070422535211\n",
      "    Current activation: sigmoid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19987/1468713152.py:57: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.labels = torch.from_numpy(self.labels.astype(np.int))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 done. loss: 0.6925538336789167, accuracy: 0.5060739436619718\n",
      "EPOCH: 1 done. loss: 0.6924389055481663, accuracy: 0.5060739436619718\n",
      "EPOCH: 2 done. loss: 0.6925772867820881, accuracy: 0.5060739436619718\n",
      "EPOCH: 3 done. loss: 0.6928304511087913, accuracy: 0.5060739436619718\n",
      "EPOCH: 4 done. loss: 0.69232974825082, accuracy: 0.5060739436619718\n",
      "EPOCH: 5 done. loss: 0.6924333124249069, accuracy: 0.5060739436619718\n",
      "EPOCH: 6 done. loss: 0.69239368836085, accuracy: 0.5060739436619718\n",
      "EPOCH: 7 done. loss: 0.6920960894337407, accuracy: 0.5060739436619718\n",
      "EPOCH: 8 done. loss: 0.6925936038847323, accuracy: 0.5060739436619718\n",
      "EPOCH: 9 done. loss: 0.6921093543370564, accuracy: 0.5060739436619718\n",
      "EPOCH: 10 done. loss: 0.692320481715379, accuracy: 0.5060739436619718\n",
      "EPOCH: 11 done. loss: 0.6919973541189124, accuracy: 0.5060739436619718\n",
      "EPOCH: 12 done. loss: 0.692447171608607, accuracy: 0.5060739436619718\n",
      "EPOCH: 13 done. loss: 0.6924990185984858, accuracy: 0.5060739436619718\n",
      "EPOCH: 14 done. loss: 0.6919779033572586, accuracy: 0.5060739436619718\n",
      "EPOCH: 15 done. loss: 0.692454281118181, accuracy: 0.5060739436619718\n",
      "EPOCH: 16 done. loss: 0.6923145022657182, accuracy: 0.5060739436619718\n",
      "EPOCH: 17 done. loss: 0.6923290879638107, accuracy: 0.5060739436619718\n",
      "EPOCH: 18 done. loss: 0.6923218195085172, accuracy: 0.5060739436619718\n",
      "EPOCH: 19 done. loss: 0.6924683687863527, accuracy: 0.5060739436619718\n",
      "EPOCH: 20 done. loss: 0.692294160525004, accuracy: 0.5060739436619718\n",
      "EPOCH: 21 done. loss: 0.6923445602258046, accuracy: 0.5060739436619718\n",
      "EPOCH: 22 done. loss: 0.6921747867707854, accuracy: 0.5060739436619718\n",
      "EPOCH: 23 done. loss: 0.6925241618244736, accuracy: 0.5060739436619718\n",
      "EPOCH: 24 done. loss: 0.6924966873946014, accuracy: 0.5060739436619718\n",
      "EPOCH: 25 done. loss: 0.6926700795138323, accuracy: 0.5060739436619718\n",
      "EPOCH: 26 done. loss: 0.6924024866686928, accuracy: 0.5060739436619718\n",
      "EPOCH: 27 done. loss: 0.6925074897430562, accuracy: 0.5060739436619718\n",
      "EPOCH: 28 done. loss: 0.6925373640325334, accuracy: 0.5060739436619718\n",
      "EPOCH: 29 done. loss: 0.6924800731517651, accuracy: 0.5060739436619718\n",
      "  Current optimizer: {'lr': 0.0005, 'weight_decay': 0.001, 'name': 'Adam'}\n",
      "    Current activation: relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19987/1468713152.py:57: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.labels = torch.from_numpy(self.labels.astype(np.int))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 done. loss: 0.5372488755870749, accuracy: 0.9630281690140845\n",
      "EPOCH: 1 done. loss: 0.5076879498031405, accuracy: 0.977112676056338\n",
      "EPOCH: 2 done. loss: 0.5055943493489866, accuracy: 0.9779929577464789\n",
      "EPOCH: 3 done. loss: 0.5034075068102942, accuracy: 0.9819542253521126\n",
      "EPOCH: 4 done. loss: 0.37028091903086063, accuracy: 0.9810739436619719\n",
      "EPOCH: 5 done. loss: 0.3241504793917691, accuracy: 0.9779929577464789\n",
      "EPOCH: 6 done. loss: 0.32180644373099004, accuracy: 0.9828345070422535\n",
      "EPOCH: 7 done. loss: 0.32157279451688126, accuracy: 0.985475352112676\n",
      "EPOCH: 8 done. loss: 0.3195883931937041, accuracy: 0.9784330985915493\n",
      "EPOCH: 9 done. loss: 0.31972920110932107, accuracy: 0.9881161971830986\n",
      "EPOCH: 10 done. loss: 0.3183627156195817, accuracy: 0.9775528169014085\n",
      "EPOCH: 11 done. loss: 0.31910093106605386, accuracy: 0.9867957746478874\n",
      "EPOCH: 12 done. loss: 0.32072575224770433, accuracy: 0.9889964788732394\n",
      "EPOCH: 13 done. loss: 0.31887065061816466, accuracy: 0.9898767605633803\n",
      "EPOCH: 14 done. loss: 0.3184371356610899, accuracy: 0.9911971830985915\n",
      "EPOCH: 15 done. loss: 0.31696497522018574, accuracy: 0.9881161971830986\n",
      "EPOCH: 16 done. loss: 0.3173658641400161, accuracy: 0.9872359154929577\n",
      "EPOCH: 17 done. loss: 0.31695333520571395, accuracy: 0.9894366197183099\n",
      "EPOCH: 18 done. loss: 0.31715581240477386, accuracy: 0.9903169014084507\n",
      "EPOCH: 19 done. loss: 0.3169859924802074, accuracy: 0.9903169014084507\n",
      "EPOCH: 20 done. loss: 0.3169701163415556, accuracy: 0.9889964788732394\n",
      "EPOCH: 21 done. loss: 0.3163897250537519, accuracy: 0.988556338028169\n",
      "EPOCH: 22 done. loss: 0.31716201570298935, accuracy: 0.9863556338028169\n",
      "EPOCH: 23 done. loss: 0.3178499206348702, accuracy: 0.9850352112676056\n",
      "EPOCH: 24 done. loss: 0.31769618458218046, accuracy: 0.988556338028169\n",
      "EPOCH: 25 done. loss: 0.3159280801260913, accuracy: 0.9889964788732394\n",
      "EPOCH: 26 done. loss: 0.31662461978417855, accuracy: 0.9898767605633803\n",
      "EPOCH: 27 done. loss: 0.31552487092989456, accuracy: 0.9863556338028169\n",
      "EPOCH: 28 done. loss: 0.31723593374093373, accuracy: 0.988556338028169\n",
      "EPOCH: 29 done. loss: 0.3175203575028313, accuracy: 0.9911971830985915\n",
      "    Current activation: sigmoid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19987/1468713152.py:57: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.labels = torch.from_numpy(self.labels.astype(np.int))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 done. loss: 0.6782912773114664, accuracy: 0.9467429577464789\n",
      "EPOCH: 1 done. loss: 0.5710561405729364, accuracy: 0.9529049295774648\n",
      "EPOCH: 2 done. loss: 0.5357837718945963, accuracy: 0.9559859154929577\n",
      "EPOCH: 3 done. loss: 0.5262286451127794, accuracy: 0.9555457746478874\n",
      "EPOCH: 4 done. loss: 0.5210210775887524, accuracy: 0.9546654929577465\n",
      "EPOCH: 5 done. loss: 0.5187968150333122, accuracy: 0.9564260563380281\n",
      "EPOCH: 6 done. loss: 0.5165737221638361, accuracy: 0.9555457746478874\n",
      "EPOCH: 7 done. loss: 0.5155986256069607, accuracy: 0.9524647887323944\n",
      "EPOCH: 8 done. loss: 0.5147344496515062, accuracy: 0.9537852112676056\n",
      "EPOCH: 9 done. loss: 0.5142574269462514, accuracy: 0.957306338028169\n",
      "EPOCH: 10 done. loss: 0.5138862941000196, accuracy: 0.9581866197183099\n",
      "EPOCH: 11 done. loss: 0.5131144662698109, accuracy: 0.9551056338028169\n",
      "EPOCH: 12 done. loss: 0.5127457088894314, accuracy: 0.9581866197183099\n",
      "EPOCH: 13 done. loss: 0.5130232822012019, accuracy: 0.9551056338028169\n",
      "EPOCH: 14 done. loss: 0.5124425971949541, accuracy: 0.9546654929577465\n",
      "EPOCH: 15 done. loss: 0.5125231055197892, accuracy: 0.9581866197183099\n",
      "EPOCH: 16 done. loss: 0.5122320535006346, accuracy: 0.9595070422535211\n",
      "EPOCH: 17 done. loss: 0.5118769931572456, accuracy: 0.9577464788732394\n",
      "EPOCH: 18 done. loss: 0.5118920650747087, accuracy: 0.9559859154929577\n",
      "EPOCH: 19 done. loss: 0.5118357217974133, accuracy: 0.9564260563380281\n",
      "EPOCH: 20 done. loss: 0.5120858543448977, accuracy: 0.9595070422535211\n",
      "EPOCH: 21 done. loss: 0.5126116632311432, accuracy: 0.9595070422535211\n",
      "EPOCH: 22 done. loss: 0.5121697499796196, accuracy: 0.9581866197183099\n",
      "EPOCH: 23 done. loss: 0.5115033443327304, accuracy: 0.9559859154929577\n",
      "EPOCH: 24 done. loss: 0.5121416487075665, accuracy: 0.9590669014084507\n",
      "EPOCH: 25 done. loss: 0.512088934911622, accuracy: 0.9608274647887324\n",
      "EPOCH: 26 done. loss: 0.5122892188805122, accuracy: 0.9590669014084507\n",
      "EPOCH: 27 done. loss: 0.5116838354755332, accuracy: 0.9568661971830986\n",
      "EPOCH: 28 done. loss: 0.5122904363605711, accuracy: 0.954225352112676\n",
      "EPOCH: 29 done. loss: 0.5123305292041214, accuracy: 0.9568661971830986\n",
      "  Current optimizer: {'lr': 0.005, 'weight_decay': 0.005, 'name': 'SGD'}\n",
      "    Current activation: relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19987/1468713152.py:57: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.labels = torch.from_numpy(self.labels.astype(np.int))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 1 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 2 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 3 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 4 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 5 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 6 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 7 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 8 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 9 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 10 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 11 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 12 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 13 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 14 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 15 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 16 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 17 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 18 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 19 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 20 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 21 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 22 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 23 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 24 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 25 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 26 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 27 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 28 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "EPOCH: 29 done. loss: 0.6931473016738892, accuracy: 0.49392605633802816\n",
      "    Current activation: sigmoid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19987/1468713152.py:57: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.labels = torch.from_numpy(self.labels.astype(np.int))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 done. loss: 0.6925583846039243, accuracy: 0.5060739436619718\n",
      "EPOCH: 1 done. loss: 0.6925633664484377, accuracy: 0.5060739436619718\n",
      "EPOCH: 2 done. loss: 0.69246602235017, accuracy: 0.5060739436619718\n",
      "EPOCH: 3 done. loss: 0.6924302452140385, accuracy: 0.5060739436619718\n",
      "EPOCH: 4 done. loss: 0.6924138243551607, accuracy: 0.5060739436619718\n",
      "EPOCH: 5 done. loss: 0.6924039242444215, accuracy: 0.5060739436619718\n",
      "EPOCH: 6 done. loss: 0.6923047862671039, accuracy: 0.5060739436619718\n",
      "EPOCH: 7 done. loss: 0.6924121117150341, accuracy: 0.5060739436619718\n",
      "EPOCH: 8 done. loss: 0.6922372822408323, accuracy: 0.5060739436619718\n",
      "EPOCH: 9 done. loss: 0.6923486274701577, accuracy: 0.5060739436619718\n",
      "EPOCH: 10 done. loss: 0.6921989425464913, accuracy: 0.5060739436619718\n",
      "EPOCH: 11 done. loss: 0.692338487174776, accuracy: 0.5060739436619718\n",
      "EPOCH: 12 done. loss: 0.692300992541843, accuracy: 0.5060739436619718\n",
      "EPOCH: 13 done. loss: 0.6921804904937745, accuracy: 0.5060739436619718\n",
      "EPOCH: 14 done. loss: 0.6923328677813212, accuracy: 0.5060739436619718\n",
      "EPOCH: 15 done. loss: 0.6922743892228161, accuracy: 0.5060739436619718\n",
      "EPOCH: 16 done. loss: 0.6922392734774836, accuracy: 0.5060739436619718\n",
      "EPOCH: 17 done. loss: 0.692203058357592, accuracy: 0.5060739436619718\n",
      "EPOCH: 18 done. loss: 0.6923307493880942, accuracy: 0.5060739436619718\n",
      "EPOCH: 19 done. loss: 0.6922362444577393, accuracy: 0.5060739436619718\n",
      "EPOCH: 20 done. loss: 0.692114606389293, accuracy: 0.5060739436619718\n",
      "EPOCH: 21 done. loss: 0.6921644897372634, accuracy: 0.5060739436619718\n",
      "EPOCH: 22 done. loss: 0.6922933216448182, accuracy: 0.5060739436619718\n",
      "EPOCH: 23 done. loss: 0.6922495409294411, accuracy: 0.5060739436619718\n",
      "EPOCH: 24 done. loss: 0.6922071116941946, accuracy: 0.5060739436619718\n",
      "EPOCH: 25 done. loss: 0.6922428162009626, accuracy: 0.5060739436619718\n",
      "EPOCH: 26 done. loss: 0.6922238718580318, accuracy: 0.5060739436619718\n",
      "EPOCH: 27 done. loss: 0.6923197421762678, accuracy: 0.5060739436619718\n",
      "EPOCH: 28 done. loss: 0.6921537946771692, accuracy: 0.5060739436619718\n",
      "EPOCH: 29 done. loss: 0.6921258149323639, accuracy: 0.5060739436619718\n",
      "  Current optimizer: {'lr': 0.0005, 'weight_decay': 0.001, 'name': 'SGD'}\n",
      "    Current activation: relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19987/1468713152.py:57: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.labels = torch.from_numpy(self.labels.astype(np.int))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 done. loss: 0.6918290279529713, accuracy: 0.5060739436619718\n",
      "EPOCH: 1 done. loss: 0.6911049524943034, accuracy: 0.5060739436619718\n",
      "EPOCH: 2 done. loss: 0.6905719567228247, accuracy: 0.5060739436619718\n",
      "EPOCH: 3 done. loss: 0.6903179444648601, accuracy: 0.5060739436619718\n",
      "EPOCH: 4 done. loss: 0.689755532918153, accuracy: 0.5060739436619718\n",
      "EPOCH: 5 done. loss: 0.6892347867842074, accuracy: 0.5060739436619718\n",
      "EPOCH: 6 done. loss: 0.6887180308500925, accuracy: 0.5060739436619718\n",
      "EPOCH: 7 done. loss: 0.6882625785138871, accuracy: 0.5065140845070423\n",
      "EPOCH: 8 done. loss: 0.6875770279654749, accuracy: 0.5065140845070423\n",
      "EPOCH: 9 done. loss: 0.686760730434347, accuracy: 0.5065140845070423\n",
      "EPOCH: 10 done. loss: 0.6858900138625392, accuracy: 0.5069542253521127\n",
      "EPOCH: 11 done. loss: 0.6848982561517646, accuracy: 0.510475352112676\n",
      "EPOCH: 12 done. loss: 0.6839289391482318, accuracy: 0.518838028169014\n",
      "EPOCH: 13 done. loss: 0.6823150566330664, accuracy: 0.5334213615023474\n",
      "EPOCH: 14 done. loss: 0.6809465710763578, accuracy: 0.557306338028169\n",
      "EPOCH: 15 done. loss: 0.6793742899541502, accuracy: 0.5899354460093896\n",
      "EPOCH: 16 done. loss: 0.6772881516703854, accuracy: 0.6300469483568075\n",
      "EPOCH: 17 done. loss: 0.6749337492165742, accuracy: 0.6630575117370893\n",
      "EPOCH: 18 done. loss: 0.6722306582662794, accuracy: 0.6969483568075118\n",
      "EPOCH: 19 done. loss: 0.6693250086572434, accuracy: 0.7242957746478873\n",
      "EPOCH: 20 done. loss: 0.6665254453818004, accuracy: 0.7493838028169014\n",
      "EPOCH: 21 done. loss: 0.6631351804291761, accuracy: 0.7749119718309859\n",
      "EPOCH: 22 done. loss: 0.659553223406827, accuracy: 0.7955985915492957\n",
      "EPOCH: 23 done. loss: 0.6553442597389222, accuracy: 0.8110035211267606\n",
      "EPOCH: 24 done. loss: 0.6510500718046118, accuracy: 0.8291079812206573\n",
      "EPOCH: 25 done. loss: 0.6463468063760687, accuracy: 0.8445129107981221\n",
      "EPOCH: 26 done. loss: 0.6406074903629444, accuracy: 0.8533157276995306\n",
      "EPOCH: 27 done. loss: 0.6346731278631421, accuracy: 0.8652582159624412\n",
      "EPOCH: 28 done. loss: 0.6279195796560358, accuracy: 0.8802230046948356\n",
      "EPOCH: 29 done. loss: 0.6201248321268293, accuracy: 0.8890258215962441\n",
      "    Current activation: sigmoid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19987/1468713152.py:57: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.labels = torch.from_numpy(self.labels.astype(np.int))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 done. loss: 0.6934244484813125, accuracy: 0.49392605633802816\n",
      "EPOCH: 1 done. loss: 0.6934124496248033, accuracy: 0.49392605633802816\n",
      "EPOCH: 2 done. loss: 0.6934131688541836, accuracy: 0.49392605633802816\n",
      "EPOCH: 3 done. loss: 0.6934063785605961, accuracy: 0.49392605633802816\n",
      "EPOCH: 4 done. loss: 0.6933747896441705, accuracy: 0.49392605633802816\n",
      "EPOCH: 5 done. loss: 0.6933692219080748, accuracy: 0.49392605633802816\n",
      "EPOCH: 6 done. loss: 0.693344721308461, accuracy: 0.49392605633802816\n",
      "EPOCH: 7 done. loss: 0.6933320656970695, accuracy: 0.49392605633802816\n",
      "EPOCH: 8 done. loss: 0.6933138895917822, accuracy: 0.49392605633802816\n",
      "EPOCH: 9 done. loss: 0.6933220512337156, accuracy: 0.49392605633802816\n",
      "EPOCH: 10 done. loss: 0.6933106852902307, accuracy: 0.49392605633802816\n",
      "EPOCH: 11 done. loss: 0.6932890185603389, accuracy: 0.49392605633802816\n",
      "EPOCH: 12 done. loss: 0.6932851720739294, accuracy: 0.49392605633802816\n",
      "EPOCH: 13 done. loss: 0.6932637393474579, accuracy: 0.49392605633802816\n",
      "EPOCH: 14 done. loss: 0.6932551695240868, accuracy: 0.49392605633802816\n",
      "EPOCH: 15 done. loss: 0.6932475474145677, accuracy: 0.49392605633802816\n",
      "EPOCH: 16 done. loss: 0.6932395054234398, accuracy: 0.49392605633802816\n",
      "EPOCH: 17 done. loss: 0.6932177713623753, accuracy: 0.49392605633802816\n",
      "EPOCH: 18 done. loss: 0.6932047819649731, accuracy: 0.49392605633802816\n",
      "EPOCH: 19 done. loss: 0.6931889291162844, accuracy: 0.49392605633802816\n",
      "EPOCH: 20 done. loss: 0.6931867597279725, accuracy: 0.49392605633802816\n",
      "EPOCH: 21 done. loss: 0.6931717210345798, accuracy: 0.49392605633802816\n",
      "EPOCH: 22 done. loss: 0.6931661418190709, accuracy: 0.49392605633802816\n",
      "EPOCH: 23 done. loss: 0.6931555445547457, accuracy: 0.49392605633802816\n",
      "EPOCH: 24 done. loss: 0.6931413063296566, accuracy: 0.49392605633802816\n",
      "EPOCH: 25 done. loss: 0.6931316466243178, accuracy: 0.5058098591549296\n",
      "EPOCH: 26 done. loss: 0.6931211317027056, accuracy: 0.6321889671361502\n",
      "EPOCH: 27 done. loss: 0.6931101973410007, accuracy: 0.5060739436619718\n",
      "EPOCH: 28 done. loss: 0.6931004817839023, accuracy: 0.5060739436619718\n",
      "EPOCH: 29 done. loss: 0.6930888579951392, accuracy: 0.5060739436619718\n",
      "best model has accuracy -1, and config:\n",
      "{'activation': 'sigmoid',\n",
      " 'cv_accuracy': 0.0,\n",
      " 'layers': [64, 32, 16],\n",
      " 'optimizer_cnf': {'lr': 0.0005, 'name': 'SGD', 'weight_decay': 0.001}}\n",
      "Test accuracy: 0.6521972132904609, f1-score: 0.7301455301455302\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'accuracy': 0.6521972132904609, 'f1-score': 0.7301455301455302}"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "from torch.nn.functional import relu\n",
    "from torch import sigmoid\n",
    "\n",
    "\n",
    "layers = [\n",
    "    [16, 16],\n",
    "    [64, 32, 16]\n",
    "]\n",
    "\n",
    "optimizers = [\n",
    "    (Adam, {\"lr\": 0.005, \"weight_decay\": 0.005}),\n",
    "    (Adam, {\"lr\": 0.0005, \"weight_decay\": 0.001}),\n",
    "    (SGD, {\"lr\": 0.005, \"weight_decay\": 0.005}),\n",
    "    (SGD, {\"lr\": 0.0005, \"weight_decay\": 0.001}),\n",
    "]\n",
    "\n",
    "activations = [relu, sigmoid]\n",
    "\n",
    "\n",
    "results, very_best_model = grid_search_nn(layers, optimizers, activations, config)\n",
    "eval_model(very_best_model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test ANN accuracy on test data - 98.7%.\n",
    "\n",
    "It's quite interesting, looks like model from time to time gets into some local minimum, and can't get out of it.\n",
    "\n",
    "Turns out used loss - cross etropy loss function ($L = \\sum_{i=1}^n y_i log(\\hat{y_i})$) is not convex[2] for weights of hidden layers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pros**\n",
    "- With more sophisticated architectures, can outperform classical approaches a lot\n",
    "- Can solve some problems previously considered too dificult for computers (computer speech, image generation, translation, etc)\n",
    "- Some deep models, like Transformers for example can generelize well for many other problems.\n",
    "\n",
    "**Cons**:\n",
    "- Quite easy to overfit\n",
    "- For greate performance requires a huge amount of data\n",
    "- Huge amount of data requires a lot of computational power and money to train a sophisticated model\n",
    "- Sometimes doesn't generelizes well\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Support Vector Machine (SVM)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "SVM is a classical model. The main idea is to find hyper-plane in a space such that best divides points into spaces.\n",
    "\n",
    "The \"best\" usually means - minimize margin (gap) formed by a hyperplane and support vectors."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### SVM as an optimization problem"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are quite a few iterative approaches and estimates that can produce solutions, but the most common way to find target hyperplance is to solve SVM as a classical optimization problem.\n",
    "\n",
    "Let $f(\\mathbf{x}) = \\mathbf{x}\\cdot \\mathbf{w} + b$ be a linear classifier predicting the responce variable $y$ via\n",
    "$$\n",
    "  y = \\operatorname{sign}(f(\\mathbf{x}))\n",
    "$$\n",
    "\n",
    "The task is to maximize the margin, i.e., the value $M$ s.t., for all $j$,\n",
    "$$\n",
    "   y_j (\\mathbf{x}_j \\cdot \\mathbf{w} + b) \\ge M \\|\\mathbf{w}\\|\n",
    "$$\n",
    "By rescaling $\\mathbf{w}$ (and $b$) so that $M\\cdot\\|\\mathbf{w}\\| = 1$, we get an equivalent problem\n",
    "\\begin{align}\n",
    "\\operatorname{argmin}_{\\mathbf{w}, b}  &\\frac12 \\|\\mathbf{w}\\|^2 \\\\\n",
    "\\text{subject to } \\qquad &y_j (\\mathbf{x}_j \\cdot \\mathbf{w} + b) \\ge 1\n",
    "\\end{align}\n",
    "\n",
    "The Lagrange function is\n",
    "$$\n",
    "  \\mathscr{L}(\\mathbf{w}, b, \\lambda) = \\frac12\\|\\mathbf{w}\\|^2 + \\sum_j \\lambda_j  [1 - y_j(\\mathbf{x}_j\\cdot \\mathbf{w} + b)]\n",
    "$$\n",
    "Partial derivatives in $\\mathbf{w}$ and $b$ give the equalities\n",
    "$$\n",
    "    \\mathbf{w} = \\sum_j \\lambda_j y_j \\mathbf{x}_j \\qquad \\text{and} \\qquad 0 = \\sum_j \\lambda_j y_j;\n",
    "$$\n",
    "we now use these in $\\mathscr{L}$ to get the Wolfe dual problem of maximizing the quadratic problem\n",
    "\\begin{equation}\n",
    "   \\mathscr{L}_D = \\sum_j \\lambda_j - \\frac12 \\sum_{j,k} \\lambda_j\\lambda_k y_jy_k \\, \\mathbf{x}_j\\cdot\\mathbf{x}_k \\tag{1}\n",
    "\\end{equation}\n",
    "subject to the restrictions $\\lambda_j \\ge 0$.\n",
    "\n",
    "In addition to the above relations, the following constraints should be satisfied for every $j$:\n",
    "$$\n",
    "   \\lambda_j [1 - y_j(\\mathbf{x}_j\\cdot \\mathbf{w} + b)] = 0\n",
    "$$\n",
    "Indices $j$ for which $y_j(\\mathbf{x}_j\\cdot \\mathbf{w} + b) = 1$ correspond to *support vectors* (i.e., points on the margin boundary).\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Soft margin SVM\n",
    "in a real life, usually data is not linearly separable. For such problem there is a so-called \"soft margin\" SVM. The idea is make some hyperparameter, that allows us to make some missclassification.\n",
    "\n",
    "To this end, we introduce slack variables $\\xi_j\\ge0$ and impose relaxed  constraints\n",
    "$$\n",
    "  y_j[\\mathbf{x}_j\\cdot \\mathbf{w} + b] \\ge 1 - \\xi_j. \\tag{2}\n",
    "$$\n",
    "The slack variables\n",
    "$$\n",
    "  \\xi_j = \\max\\{0, 1- y_j[\\mathbf{x}_j \\cdot \\mathbf{w} + b]\\}\n",
    "$$\n",
    "measure incorrectness of classification: $\\xi_j = 0$ for correctly classified points outside the slab and $\\xi_j >0$ for points that fall inside the slab or are in the wrong halfspace. The new objective function is\n",
    "$$\n",
    "  f(\\mathbf{w},b,\\xi_j) = \\frac12\\|\\mathbf{w}\\|^2 + C \\sum_j \\xi_j\n",
    "$$\n",
    "under the constraints (2); here $C$ is the cost constant penalizing misclassification."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Gradient descent\n",
    "After we calculated cost function, we can calculate gradients, and solve the problem with gradient descent.\n",
    "\n",
    "$$\\nabla_{\\mathbf{w}}g = \\mathbf{w} - C \\sum_{\\xi_j>0} y_j x_j$$\n",
    "\n",
    "$$\\frac{\\partial g}{\\partial b} = -C \\sum_{\\xi_j>0}  y_j$$\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Kernel trick"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The idea of the trick - map data into some higher dimensional space. Not separable data may become linearly separable.\n",
    "\n",
    "As we can see in the Wolfe problem, we need to only define a dot product of vectors.\n",
    "\n",
    "\\begin{equation}\n",
    "   \\mathscr{L}_D = \\sum_j \\lambda_j - \\frac12 \\sum_{j,k} \\lambda_j\\lambda_k y_jy_k \\, \\mathbf{x}_j\\cdot\\mathbf{x}_k \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "That's why we can use a kernel function, that is just a dot product of two mapped vectors $K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)$\n",
    "\n",
    "So new Wolfe problem:\n",
    "$$\n",
    "  \\mathscr{L}_D = \\sum_j \\lambda_j - \\frac12 \\sum_{j,k} \\lambda_j\\lambda_k y_jy_k \\, K(\\mathbf{x}_j, \\mathbf{x}_k)\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation accuracy=0.8723, parameters={'kernel': 'sigmoid', 'degree': 3}\n",
      "Cross-validation accuracy=0.8723, parameters={'kernel': 'sigmoid', 'degree': 4}\n",
      "Cross-validation accuracy=0.8723, parameters={'kernel': 'sigmoid', 'degree': 5}\n",
      "Cross-validation accuracy=0.9920, parameters={'kernel': 'rbf', 'degree': 3}\n",
      "Cross-validation accuracy=0.9920, parameters={'kernel': 'rbf', 'degree': 4}\n",
      "Cross-validation accuracy=0.9920, parameters={'kernel': 'rbf', 'degree': 5}\n",
      "Cross-validation accuracy=0.9894, parameters={'kernel': 'poly', 'degree': 3}\n",
      "Cross-validation accuracy=0.9849, parameters={'kernel': 'poly', 'degree': 4}\n",
      "Cross-validation accuracy=0.9734, parameters={'kernel': 'poly', 'degree': 5}\n",
      "Test accuracy=0.9941, parameters={'kernel': 'rbf', 'degree': 3}\n"
     ]
    },
    {
     "data": {
      "text/plain": "SVC()"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "parameters = {\n",
    "    'kernel': ['sigmoid', 'rbf', 'poly'],\n",
    "    'degree': [3, 4, 5],\n",
    "}\n",
    "select_model(dataset, SVC, parameters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pros**:\n",
    "- Works well, when data is easyly separable\n",
    "- Effective in a high dimensional space\n",
    "- Works well, when the number of dimensions os bigger than number of samples\n",
    "- Memory efficient\n",
    "\n",
    "**Cons**:\n",
    "- Doesn't work well, when data is not easyly separable and noisy\n",
    "- Training time is O(n^2), so it's not suitable for huge datasets\n",
    "- Model is not probabilistic, we label points with just 1/-1.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test SVM accuracy on test data - 99.4%."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Rank:**\n",
    "1. SVM - 99.4%\n",
    "2. k-NN - 98.8%\n",
    "3. ANN - 98.7%\n",
    "4. Logistic regression - 95.6%\n",
    "\n",
    "In this report, we've tried to consider classification approaches that are as different as possible. I think that almost accomplished it except for ANN and Logistic regression, where the last one can be viewed as a simplified version of ANN. Overall, I think we've managed to get pretty good accuracy for most of the methods and discussed reasons for logistic regression's poor performance. To be honest, the final ranking of the models was a little surprising for us - that's, I believe, a good lesson showing why in machine learning, you must never stick to some favorite model without testing others."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### References\n",
    "[1] - Artificial neural network - [Wikipedia](https://en.wikipedia.org/wiki/Artificial_neural_network)\n",
    "[2] - Is Cross entropy cost function for neural network convex? - [StackExchange](https://stats.stackexchange.com/questions/144378/is-cross-entropy-cost-function-for-neural-network-convex)\n",
    "[3] - BackPropagation - [Wikipedia](https://en.wikipedia.org/wiki/Backpropagation)\n",
    "[4] - What is Support Vector Machine (SVM)? - [Medium](https://medium.com/@cdabakoglu/what-is-support-vector-machine-svm-fd0e9e39514f)\n",
    "[5] - Lab work #2 SVM - MMML course.\n",
    "[6] - Understanding Support Vector Machine(SVM) algorithm from examples - [Analytics Vidhuya](https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/)\n",
    "[7] - Universal approximation theorem - [Wikipedia](https://en.wikipedia.org/wiki/Universal_approximation_theorem)\n",
    "[8] - Conceptual Understanding of Logistic Regression for Data Science Beginners - [Analytics Vidhuya](https://www.analyticsvidhya.com/blog/2021/08/conceptual-understanding-of-logistic-regression-for-data-science-beginners/#h2_5)\n",
    "[9] - Advantages and Disadvantages of Logistic Regression - [OpenGenus](https://iq.opengenus.org/advantages-and-disadvantages-of-logistic-regression/)\n",
    "[10] - k-nearest neighbors - [CS.Cornell](https://www.cs.cornell.edu/courses/cs4780/2017sp/lectures/lecturenote02_kNN.html)\n",
    "[11] - Pros and Cons of K-Nearest Neighbors - [Genesis](https://www.fromthegenesis.com/pros-and-cons-of-k-nearest-neighbors/)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}